{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90bff9d0",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bbdbdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce07718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Misc\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import warnings\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "# Data management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sound treatments\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Class weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Concatenate\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# TRILL\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.enable_v2_behavior()\n",
    "assert tf.executing_eagerly()\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# EfficientNetB0\n",
    "from keras.applications.efficientnet import EfficientNetB0\n",
    "from keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "# VGGish\n",
    "from vggish import vggish_input\n",
    "from vggish import vggish_params as params\n",
    "import vggish_keras as vgk\n",
    "\n",
    "# Meta model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## Metrics\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.layers.netvlad import NetVLAD\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577e8401",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69cc41a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.6.0\n",
      "Default GPU Device: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Inactivate warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Allow to display all dataframes columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Display Tensorlfow version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print('No GPU found. Please ensure you have installed TensorFlow correctly')\n",
    "    \n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(\n",
    "        tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "    # Allow memory growth\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53f27839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_PATH = '/kaggle/input/birdclef-2022/'\n",
    "#WORKING_PATH = '/kaggle/working/'\n",
    "#MODEL_PATH = '/kaggle/input/kernel-efficientnetb0-melspec/'\n",
    "\n",
    "DATA_PATH = './data/'\n",
    "WORKING_PATH = './working/stacking/'\n",
    "MODEL_PATH = './working/stacking/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4383306",
   "metadata": {},
   "source": [
    "# Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d85f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator_trill(Sequence):\n",
    "    def __init__(self,\n",
    "                 _X,\n",
    "                 batch_size=32,\n",
    "                 n_channels=1,\n",
    "                 n_columns=470,\n",
    "                 n_rows=120,\n",
    "                 shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.X = _X\n",
    "        self.n_channels = n_channels\n",
    "        self.n_columns = n_columns\n",
    "        self.n_rows = n_rows\n",
    "        self.shuffle = shuffle\n",
    "        self.img_indexes = np.arange(len(self.X))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.img_indexes) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temps = [self.img_indexes[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temps)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.X))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temps):\n",
    "        X = np.empty((self.batch_size, 80000))\n",
    "        y = np.empty((self.batch_size, 21), dtype=int)\n",
    "        for i, ID in enumerate(list_IDs_temps):\n",
    "            file_path = self.X.iloc[ID]['filename']\n",
    "\n",
    "            #audio, sr = librosa.load(file_path)\n",
    "            #feat = extractFeatures(audio, sr)\n",
    "\n",
    "            feat = data_mem[file_path]\n",
    "\n",
    "            x_features = feat.tolist()\n",
    "            label = self.X.iloc[ID]['target']\n",
    "            X[i] = np.array(x_features)\n",
    "            y[i] = mlb.transform([label])\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aad1abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator_EfficientNetB0(Sequence):\n",
    "    def __init__(self,\n",
    "                 _X,\n",
    "                 batch_size=32,\n",
    "                 n_channels=1,\n",
    "                 n_columns=470,\n",
    "                 n_rows=120,\n",
    "                 shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.X = _X\n",
    "        self.n_channels = n_channels\n",
    "        self.n_columns = n_columns\n",
    "        self.n_rows = n_rows\n",
    "        self.shuffle = shuffle\n",
    "        self.img_indexes = np.arange(len(self.X))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.img_indexes) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temps = [self.img_indexes[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temps)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.X))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temps):\n",
    "        X = np.empty((self.batch_size, self.n_rows, self.n_columns, self.n_channels))\n",
    "        y = np.empty((self.batch_size, len(mlb.classes_)), dtype=int)\n",
    "        for i, ID in enumerate(list_IDs_temps):\n",
    "            file_path = self.X.iloc[ID]['filename']\n",
    "            \n",
    "            #audio, sr = librosa.load(file_path)\n",
    "            #feat = extractFeatures(audio, sr)\n",
    "            \n",
    "            feat = data_mem[file_path]\n",
    "            \n",
    "            #x_features = feat.tolist()\n",
    "            label = self.X.iloc[ID]['target']\n",
    "            #X[i] = np.array(x_features)\n",
    "            X[i] = feat\n",
    "            y[i] = mlb.transform([label])\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caad47dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator_VGGish(Sequence):\n",
    "    def __init__(self,\n",
    "                 _X,\n",
    "                 batch_size=32,\n",
    "                 n_channels=1,\n",
    "                 n_columns=470,\n",
    "                 n_rows=120,\n",
    "                 shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.X = _X\n",
    "        self.n_channels = n_channels\n",
    "        self.n_columns = n_columns\n",
    "        self.n_rows = n_rows\n",
    "        self.shuffle = shuffle\n",
    "        self.img_indexes = np.arange(len(self.X))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.img_indexes) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temps = [self.img_indexes[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temps)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.X))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temps):\n",
    "        X = np.empty((self.batch_size, self.n_rows, self.n_columns, self.n_channels))\n",
    "        y = np.empty((self.batch_size, len(mlb.classes_)), dtype=int)\n",
    "        for i, ID in enumerate(list_IDs_temps):\n",
    "            file_path = self.X.iloc[ID]['filename']\n",
    "            \n",
    "            #audio, sr = librosa.load(file_path)\n",
    "            #feat = extractFeatures(audio, sr)\n",
    "            \n",
    "            feat = data_mem[file_path]\n",
    "            \n",
    "            x_features = feat.tolist()\n",
    "            label = self.X.iloc[ID]['target']\n",
    "            X[i] = np.array(x_features)\n",
    "            y[i] = mlb.transform([label])\n",
    "        X = X.reshape(X.shape[0], self.n_rows, self.n_columns, self.n_channels)\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b2f262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mem = {}\n",
    "\n",
    "def LoadRAM():\n",
    "    # Load extracted features into RAM\n",
    "    data_mem.clear()\n",
    "\n",
    "    # Instantiate the progress bar\n",
    "    max_count = data_df.shape[0]\n",
    "    f = IntProgress(min=0, max=max_count)\n",
    "    # Display the progress bar\n",
    "    display(f)\n",
    "\n",
    "    temp = {}\n",
    "\n",
    "    for index, row in data_df.iterrows():\n",
    "        # Increment the progress bar\n",
    "        f.value += 1\n",
    "\n",
    "        # Get file path\n",
    "        file_path = row['filename']\n",
    "        # Load audio file\n",
    "        audio, sr = librosa.load(file_path)\n",
    "        # Extracxt features\n",
    "        feat = extractFeatures(audio, sr)\n",
    "        \n",
    "        # Store features into the dedicated dictionary\n",
    "        temp[row['filename']] = feat\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0270101",
   "metadata": {},
   "source": [
    "# Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c081303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['akiapo',\n",
       " 'aniani',\n",
       " 'apapan',\n",
       " 'barpet',\n",
       " 'crehon',\n",
       " 'elepai',\n",
       " 'ercfra',\n",
       " 'hawama',\n",
       " 'hawcre',\n",
       " 'hawgoo',\n",
       " 'hawhaw',\n",
       " 'hawpet1',\n",
       " 'houfin',\n",
       " 'iiwi',\n",
       " 'jabwar',\n",
       " 'maupar',\n",
       " 'omao',\n",
       " 'puaioh',\n",
       " 'skylar',\n",
       " 'warwhe1',\n",
       " 'yefcan']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load meta data\n",
    "train_meta = pd.read_csv(DATA_PATH + 'train_metadata.csv')\n",
    "\n",
    "# Load scored birds\n",
    "with open(DATA_PATH + 'scored_birds.json') as sbfile:\n",
    "    scored_birds = json.load(sbfile)\n",
    "    \n",
    "# Focus on 21 scored classes\n",
    "labels = list(train_meta[train_meta['primary_label'].isin(scored_birds)]['primary_label'].unique())\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b25f076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(WORKING_PATH + 'data_5_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e3122b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def to_list(df):\n",
    "    temp = []\n",
    "    primary_label = df['primary_label']\n",
    "    \n",
    "    if df['secondary_labels'] != '[]':\n",
    "        secondary_labels = df['secondary_labels'].replace(\n",
    "            '[', '').replace(']', '').replace(\"'\", '').replace(' ', '').split(',')\n",
    "    else:\n",
    "        secondary_labels = None\n",
    "\n",
    "    temp.append(primary_label)\n",
    "    \n",
    "    if secondary_labels != None:\n",
    "        for item in secondary_labels:\n",
    "            if item in labels:\n",
    "                if item not in temp:\n",
    "                    temp.append(item)\n",
    "                \n",
    "    return tuple(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8bb07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target\n",
    "data_df['target'] = data_df.apply(to_list, axis=1)\n",
    "data_df.to_pickle(WORKING_PATH + 'data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fde482f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>secondary_labels</th>\n",
       "      <th>original_filename</th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>akiapo</td>\n",
       "      <td>['apapan', 'hawama', 'iiwi']</td>\n",
       "      <td>akiapo/XC122399.ogg</td>\n",
       "      <td>./working/step3/each5s/split_1_akiapo_XC122399...</td>\n",
       "      <td>(akiapo, apapan, hawama, iiwi)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>akiapo</td>\n",
       "      <td>['apapan', 'hawama', 'iiwi']</td>\n",
       "      <td>akiapo/XC122399.ogg</td>\n",
       "      <td>./working/step3/each5s/split_2_akiapo_XC122399...</td>\n",
       "      <td>(akiapo, apapan, hawama, iiwi)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>akiapo</td>\n",
       "      <td>['apapan', 'hawama', 'iiwi']</td>\n",
       "      <td>akiapo/XC122399.ogg</td>\n",
       "      <td>./working/step3/each5s/split_3_akiapo_XC122399...</td>\n",
       "      <td>(akiapo, apapan, hawama, iiwi)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>akiapo</td>\n",
       "      <td>['apapan', 'hawama', 'iiwi']</td>\n",
       "      <td>akiapo/XC122399.ogg</td>\n",
       "      <td>./working/step3/each5s/split_4_akiapo_XC122399...</td>\n",
       "      <td>(akiapo, apapan, hawama, iiwi)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>akiapo</td>\n",
       "      <td>['apapan', 'hawama', 'iiwi']</td>\n",
       "      <td>akiapo/XC122399.ogg</td>\n",
       "      <td>./working/step3/each5s/split_5_akiapo_XC122399...</td>\n",
       "      <td>(akiapo, apapan, hawama, iiwi)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14075</th>\n",
       "      <td>yefcan</td>\n",
       "      <td>[]</td>\n",
       "      <td>yefcan/XC667142.ogg</td>\n",
       "      <td>./working/step3/each5s/split_3_yefcan_XC667142...</td>\n",
       "      <td>(yefcan,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14076</th>\n",
       "      <td>yefcan</td>\n",
       "      <td>[]</td>\n",
       "      <td>yefcan/XC667142.ogg</td>\n",
       "      <td>./working/step3/each5s/split_4_yefcan_XC667142...</td>\n",
       "      <td>(yefcan,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14077</th>\n",
       "      <td>yefcan</td>\n",
       "      <td>[]</td>\n",
       "      <td>yefcan/XC667142.ogg</td>\n",
       "      <td>./working/step3/each5s/split_5_yefcan_XC667142...</td>\n",
       "      <td>(yefcan,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14078</th>\n",
       "      <td>yefcan</td>\n",
       "      <td>[]</td>\n",
       "      <td>yefcan/XC667142.ogg</td>\n",
       "      <td>./working/step3/each5s/split_6_yefcan_XC667142...</td>\n",
       "      <td>(yefcan,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14079</th>\n",
       "      <td>yefcan</td>\n",
       "      <td>[]</td>\n",
       "      <td>yefcan/XC667142.ogg</td>\n",
       "      <td>./working/step3/each5s/split_7_yefcan_XC667142...</td>\n",
       "      <td>(yefcan,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14080 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      primary_label              secondary_labels    original_filename  \\\n",
       "0            akiapo  ['apapan', 'hawama', 'iiwi']  akiapo/XC122399.ogg   \n",
       "1            akiapo  ['apapan', 'hawama', 'iiwi']  akiapo/XC122399.ogg   \n",
       "2            akiapo  ['apapan', 'hawama', 'iiwi']  akiapo/XC122399.ogg   \n",
       "3            akiapo  ['apapan', 'hawama', 'iiwi']  akiapo/XC122399.ogg   \n",
       "4            akiapo  ['apapan', 'hawama', 'iiwi']  akiapo/XC122399.ogg   \n",
       "...             ...                           ...                  ...   \n",
       "14075        yefcan                            []  yefcan/XC667142.ogg   \n",
       "14076        yefcan                            []  yefcan/XC667142.ogg   \n",
       "14077        yefcan                            []  yefcan/XC667142.ogg   \n",
       "14078        yefcan                            []  yefcan/XC667142.ogg   \n",
       "14079        yefcan                            []  yefcan/XC667142.ogg   \n",
       "\n",
       "                                                filename  \\\n",
       "0      ./working/step3/each5s/split_1_akiapo_XC122399...   \n",
       "1      ./working/step3/each5s/split_2_akiapo_XC122399...   \n",
       "2      ./working/step3/each5s/split_3_akiapo_XC122399...   \n",
       "3      ./working/step3/each5s/split_4_akiapo_XC122399...   \n",
       "4      ./working/step3/each5s/split_5_akiapo_XC122399...   \n",
       "...                                                  ...   \n",
       "14075  ./working/step3/each5s/split_3_yefcan_XC667142...   \n",
       "14076  ./working/step3/each5s/split_4_yefcan_XC667142...   \n",
       "14077  ./working/step3/each5s/split_5_yefcan_XC667142...   \n",
       "14078  ./working/step3/each5s/split_6_yefcan_XC667142...   \n",
       "14079  ./working/step3/each5s/split_7_yefcan_XC667142...   \n",
       "\n",
       "                               target  \n",
       "0      (akiapo, apapan, hawama, iiwi)  \n",
       "1      (akiapo, apapan, hawama, iiwi)  \n",
       "2      (akiapo, apapan, hawama, iiwi)  \n",
       "3      (akiapo, apapan, hawama, iiwi)  \n",
       "4      (akiapo, apapan, hawama, iiwi)  \n",
       "...                               ...  \n",
       "14075                       (yefcan,)  \n",
       "14076                       (yefcan,)  \n",
       "14077                       (yefcan,)  \n",
       "14078                       (yefcan,)  \n",
       "14079                       (yefcan,)  \n",
       "\n",
       "[14080 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_pickle(WORKING_PATH + 'data.pkl')\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d2ebcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(data_df['target'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6944fb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['akiapo', 'aniani', 'apapan', 'barpet', 'crehon', 'elepai',\n",
       "       'ercfra', 'hawama', 'hawcre', 'hawgoo', 'hawhaw', 'hawpet1',\n",
       "       'houfin', 'iiwi', 'jabwar', 'maupar', 'omao', 'puaioh', 'skylar',\n",
       "       'warwhe1', 'yefcan'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f659c",
   "metadata": {},
   "source": [
    "# Classes weight management function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a51b2e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_weight(generator, mu=0.15):\n",
    "    weights = {}\n",
    "\n",
    "    labels_dict = {}\n",
    "    count_class = 0\n",
    "    for item in mlb.classes_:\n",
    "        labels_dict[count_class] = 0\n",
    "\n",
    "        for index, row in generator.X.iterrows():\n",
    "            if item in row['target']:\n",
    "                labels_dict[count_class] += 1\n",
    "                \n",
    "        count_class += 1\n",
    "\n",
    "    total = sum(labels_dict.values())\n",
    "    keys = labels_dict.keys()\n",
    "\n",
    "    for i in sorted(keys):\n",
    "        score = np.log(0.85*total/float(labels_dict[i]))\n",
    "        weights[i] = score if score > 1 else 1\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04729c09",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fada22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "es_callback = EarlyStopping(monitor='val_loss',\n",
    "                            mode='min',\n",
    "                            patience=5,\n",
    "                            verbose=1,\n",
    "                            restore_best_weights=True\n",
    "                            )\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.8,\n",
    "                              mode='min',\n",
    "                              patience=1,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001,\n",
    "                              cooldown=1,\n",
    "                              min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a46a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "X_train, X_valid, _, _ = train_test_split(\n",
    "    data_df, data_df['target'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd00cf8e",
   "metadata": {},
   "source": [
    "## Trill-distilled/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063c9590",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59fb2514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sound noise reduction\n",
    "def f_high(y,sr):\n",
    "    b,a = signal.butter(10, 1000/(sr/2), btype='highpass')\n",
    "    yf = signal.lfilter(b,a,y)\n",
    "    return yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3b43d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(y, sr):\n",
    "    # Sound noise reduction\n",
    "    y = f_high(y, sr)\n",
    "    # Resample\n",
    "    y = librosa.resample(y, sr, 16000)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45c8d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    batch_size=32,\n",
    "    n_rows=224,\n",
    "    n_columns=216,\n",
    "    n_channels=3,\n",
    ")\n",
    "params_train = dict(\n",
    "    shuffle=False,\n",
    "    **params\n",
    ")\n",
    "params_valid = dict(\n",
    "    shuffle=False,\n",
    "    **params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c313a41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed363b781a14077b10d2932cc361c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=14080)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data in RAM to speed up training process\n",
    "data_mem = LoadRAM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e524eccf",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65633fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_model(num_classes, input_length, use_batchnorm=True, l2=1e-5,\n",
    "                    num_clusters=None, trainable=True, pooling='avg', hidden=0):\n",
    "    \"\"\"Make a model.\"\"\"\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.Input((input_length,)))\n",
    "    \n",
    "    # 'https://tfhub.dev/google/nonsemantic-speech-benchmark/trill-distilled/3'\n",
    "    trill_layer = hub.KerasLayer(\n",
    "        handle=MODEL_PATH + 'trill/',\n",
    "        trainable=trainable,\n",
    "        arguments={'sample_rate': int(16000)},\n",
    "        output_key='embedding',\n",
    "        output_shape=[None, 2048]\n",
    "    )\n",
    "    \n",
    "    model.add(trill_layer)\n",
    "    \n",
    "    if num_clusters and num_clusters > 0:\n",
    "        model.add(NetVLAD(num_clusters=num_clusters))\n",
    "        if use_batchnorm:\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            # Average pooling\n",
    "            model.add(tf.keras.layers.GlobalAveragePooling1D())  \n",
    "        else:\n",
    "            model.add(tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1)))\n",
    "       \n",
    "    # Hidden layer\n",
    "    if hidden != 0:\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "            hidden, \n",
    "            activation='relu'))\n",
    "    \n",
    "    # Fully connected\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        num_classes, \n",
    "        activation='sigmoid',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(l=l2)))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f9258a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(num_clusters, use_batchnorm, pooling, hidden, fine_tune_at, model_path):\n",
    "    if fine_tune_at == None:\n",
    "        print('fine_tune_at == None')\n",
    "        model = get_keras_model(len(labels), \n",
    "                                80000, \n",
    "                                use_batchnorm=use_batchnorm, \n",
    "                                l2=1e-5,\n",
    "                                num_clusters=num_clusters, \n",
    "                                trainable=False,\n",
    "                                pooling=pooling,\n",
    "                                hidden=hidden\n",
    "                               )\n",
    "\n",
    "    else:\n",
    "        print('model.load_weights')\n",
    "        model = get_keras_model(len(labels), \n",
    "                                80000, \n",
    "                                use_batchnorm=use_batchnorm, \n",
    "                                l2=1e-5,\n",
    "                                num_clusters=num_clusters, \n",
    "                                trainable=True,\n",
    "                                pooling=pooling,\n",
    "                                hidden=hidden\n",
    "                               )\n",
    "\n",
    "        # Load existing weights\n",
    "        model.load_weights(model_path)\n",
    "\n",
    "    print('')\n",
    "    model.summary()\n",
    "    print('')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58345741",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights...\n",
      "Create model...\n",
      "fine_tune_at == None\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_1 (KerasLayer)   (None, None, 2048)        51964864  \n",
      "_________________________________________________________________\n",
      "net_vlad_1 (NetVLAD)         (None, 16384)             32776     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16384)             65536     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               4194560   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 21)                5397      \n",
      "=================================================================\n",
      "Total params: 56,263,133\n",
      "Trainable params: 4,265,501\n",
      "Non-trainable params: 51,997,632\n",
      "_________________________________________________________________\n",
      "\n",
      "fit...\n",
      "Epoch 1/50\n",
      "281/281 [==============================] - 55s 188ms/step - loss: 0.2430 - f1macro: 0.2910 - val_loss: 0.1485 - val_f1macro: 0.0596\n",
      "Epoch 2/50\n",
      "281/281 [==============================] - 52s 185ms/step - loss: 0.1444 - f1macro: 0.4998 - val_loss: 0.0909 - val_f1macro: 0.2865\n",
      "Epoch 3/50\n",
      "281/281 [==============================] - 51s 182ms/step - loss: 0.1078 - f1macro: 0.5880 - val_loss: 0.0512 - val_f1macro: 0.5316\n",
      "Epoch 4/50\n",
      "281/281 [==============================] - 51s 182ms/step - loss: 0.0811 - f1macro: 0.6377 - val_loss: 0.0521 - val_f1macro: 0.5408\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 5/50\n",
      "281/281 [==============================] - 52s 184ms/step - loss: 0.0540 - f1macro: 0.7010 - val_loss: 0.0444 - val_f1macro: 0.5885\n",
      "Epoch 6/50\n",
      "281/281 [==============================] - 52s 183ms/step - loss: 0.0416 - f1macro: 0.7154 - val_loss: 0.0424 - val_f1macro: 0.5696\n",
      "Epoch 7/50\n",
      "281/281 [==============================] - 51s 181ms/step - loss: 0.0327 - f1macro: 0.7044 - val_loss: 0.0427 - val_f1macro: 0.5953\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 8/50\n",
      "281/281 [==============================] - 51s 181ms/step - loss: 0.0202 - f1macro: 0.7217 - val_loss: 0.0437 - val_f1macro: 0.5620\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 9/50\n",
      "281/281 [==============================] - 51s 182ms/step - loss: 0.0144 - f1macro: 0.7225 - val_loss: 0.0390 - val_f1macro: 0.6202\n",
      "Epoch 10/50\n",
      "281/281 [==============================] - 51s 183ms/step - loss: 0.0109 - f1macro: 0.7453 - val_loss: 0.0410 - val_f1macro: 0.5958\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "Epoch 11/50\n",
      "281/281 [==============================] - 52s 186ms/step - loss: 0.0091 - f1macro: 0.7244 - val_loss: 0.0407 - val_f1macro: 0.6133\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "Epoch 12/50\n",
      "281/281 [==============================] - 52s 187ms/step - loss: 0.0066 - f1macro: 0.7418 - val_loss: 0.0412 - val_f1macro: 0.6255\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "Epoch 13/50\n",
      "281/281 [==============================] - 52s 186ms/step - loss: 0.0056 - f1macro: 0.7476 - val_loss: 0.0412 - val_f1macro: 0.6455\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "Epoch 14/50\n",
      "281/281 [==============================] - 52s 186ms/step - loss: 0.0050 - f1macro: 0.7499 - val_loss: 0.0408 - val_f1macro: 0.6216\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "Epoch 00014: early stopping\n",
      "predict_on_batch...\n",
      "Class weights...\n",
      "Create model...\n",
      "fine_tune_at == None\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_2 (KerasLayer)   (None, None, 2048)        51964864  \n",
      "_________________________________________________________________\n",
      "net_vlad_2 (NetVLAD)         (None, 16384)             32776     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16384)             65536     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               4194560   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 21)                5397      \n",
      "=================================================================\n",
      "Total params: 56,263,133\n",
      "Trainable params: 4,265,501\n",
      "Non-trainable params: 51,997,632\n",
      "_________________________________________________________________\n",
      "\n",
      "fit...\n",
      "Epoch 1/50\n",
      "281/281 [==============================] - 52s 179ms/step - loss: 0.2341 - f1macro: 0.3071 - val_loss: 0.1567 - val_f1macro: 0.0772\n",
      "Epoch 2/50\n",
      "281/281 [==============================] - 50s 178ms/step - loss: 0.1403 - f1macro: 0.4742 - val_loss: 0.0810 - val_f1macro: 0.4152\n",
      "Epoch 3/50\n",
      "281/281 [==============================] - 51s 181ms/step - loss: 0.1029 - f1macro: 0.5688 - val_loss: 0.0521 - val_f1macro: 0.5993\n",
      "Epoch 4/50\n",
      "281/281 [==============================] - 50s 179ms/step - loss: 0.0777 - f1macro: 0.6147 - val_loss: 0.0494 - val_f1macro: 0.6123\n",
      "Epoch 5/50\n",
      "281/281 [==============================] - 50s 179ms/step - loss: 0.0570 - f1macro: 0.6821 - val_loss: 0.0436 - val_f1macro: 0.6160\n",
      "Epoch 6/50\n",
      "281/281 [==============================] - 52s 185ms/step - loss: 0.0435 - f1macro: 0.6987 - val_loss: 0.0534 - val_f1macro: 0.5174\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 7/50\n",
      "281/281 [==============================] - 52s 184ms/step - loss: 0.0286 - f1macro: 0.6918 - val_loss: 0.0421 - val_f1macro: 0.6117\n",
      "Epoch 8/50\n",
      "281/281 [==============================] - 51s 182ms/step - loss: 0.0207 - f1macro: 0.7119 - val_loss: 0.0423 - val_f1macro: 0.6316\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 9/50\n",
      "281/281 [==============================] - 55s 197ms/step - loss: 0.0139 - f1macro: 0.6937 - val_loss: 0.0410 - val_f1macro: 0.6018\n",
      "Epoch 10/50\n",
      "281/281 [==============================] - 53s 188ms/step - loss: 0.0107 - f1macro: 0.6991 - val_loss: 0.0428 - val_f1macro: 0.6630\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 11/50\n",
      "281/281 [==============================] - 52s 184ms/step - loss: 0.0078 - f1macro: 0.7110 - val_loss: 0.0403 - val_f1macro: 0.6266\n",
      "Epoch 12/50\n",
      "281/281 [==============================] - 52s 184ms/step - loss: 0.0067 - f1macro: 0.7011 - val_loss: 0.0420 - val_f1macro: 0.6437\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "Epoch 13/50\n",
      "281/281 [==============================] - 52s 186ms/step - loss: 0.0052 - f1macro: 0.7195 - val_loss: 0.0413 - val_f1macro: 0.6612\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "Epoch 14/50\n",
      "281/281 [==============================] - 52s 185ms/step - loss: 0.0046 - f1macro: 0.7200 - val_loss: 0.0418 - val_f1macro: 0.6524\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "Epoch 15/50\n",
      "281/281 [==============================] - 57s 202ms/step - loss: 0.0041 - f1macro: 0.7259 - val_loss: 0.0426 - val_f1macro: 0.6620\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "Epoch 16/50\n",
      "281/281 [==============================] - 53s 189ms/step - loss: 0.0038 - f1macro: 0.7291 - val_loss: 0.0433 - val_f1macro: 0.6518\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "Epoch 00016: early stopping\n",
      "predict_on_batch...\n",
      "Class weights...\n",
      "Create model...\n",
      "fine_tune_at == None\n",
      "\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_3 (KerasLayer)   (None, None, 2048)        51964864  \n",
      "_________________________________________________________________\n",
      "net_vlad_3 (NetVLAD)         (None, 16384)             32776     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16384)             65536     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               4194560   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 21)                5397      \n",
      "=================================================================\n",
      "Total params: 56,263,133\n",
      "Trainable params: 4,265,501\n",
      "Non-trainable params: 51,997,632\n",
      "_________________________________________________________________\n",
      "\n",
      "fit...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "281/281 [==============================] - 54s 184ms/step - loss: 0.2305 - f1macro: 0.3095 - val_loss: 0.1580 - val_f1macro: 0.0853\n",
      "Epoch 2/50\n",
      "281/281 [==============================] - 52s 183ms/step - loss: 0.1399 - f1macro: 0.5178 - val_loss: 0.0769 - val_f1macro: 0.4588\n",
      "Epoch 3/50\n",
      "281/281 [==============================] - 54s 191ms/step - loss: 0.1028 - f1macro: 0.6079 - val_loss: 0.0519 - val_f1macro: 0.6065\n",
      "Epoch 4/50\n",
      "281/281 [==============================] - 52s 184ms/step - loss: 0.0762 - f1macro: 0.6505 - val_loss: 0.0401 - val_f1macro: 0.5958\n",
      "Epoch 5/50\n",
      "281/281 [==============================] - 53s 187ms/step - loss: 0.0568 - f1macro: 0.7079 - val_loss: 0.0442 - val_f1macro: 0.5416\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 6/50\n",
      "281/281 [==============================] - 54s 193ms/step - loss: 0.0378 - f1macro: 0.7170 - val_loss: 0.0466 - val_f1macro: 0.5590\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 7/50\n",
      "281/281 [==============================] - 54s 193ms/step - loss: 0.0238 - f1macro: 0.7264 - val_loss: 0.0444 - val_f1macro: 0.5609\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 8/50\n",
      "281/281 [==============================] - 55s 197ms/step - loss: 0.0154 - f1macro: 0.7431 - val_loss: 0.0376 - val_f1macro: 0.5725\n",
      "Epoch 9/50\n",
      "281/281 [==============================] - 57s 204ms/step - loss: 0.0119 - f1macro: 0.7180 - val_loss: 0.0391 - val_f1macro: 0.5612\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "Epoch 10/50\n",
      "281/281 [==============================] - 54s 192ms/step - loss: 0.0092 - f1macro: 0.7260 - val_loss: 0.0381 - val_f1macro: 0.5873\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "Epoch 11/50\n",
      "281/281 [==============================] - 54s 192ms/step - loss: 0.0068 - f1macro: 0.7195 - val_loss: 0.0391 - val_f1macro: 0.5497\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "Epoch 12/50\n",
      "281/281 [==============================] - 54s 192ms/step - loss: 0.0058 - f1macro: 0.7297 - val_loss: 0.0377 - val_f1macro: 0.5646\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "Epoch 13/50\n",
      "281/281 [==============================] - 54s 191ms/step - loss: 0.0049 - f1macro: 0.7391 - val_loss: 0.0381 - val_f1macro: 0.5699\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "Epoch 00013: early stopping\n",
      "predict_on_batch...\n",
      "Class weights...\n",
      "Create model...\n",
      "fine_tune_at == None\n",
      "\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_4 (KerasLayer)   (None, None, 2048)        51964864  \n",
      "_________________________________________________________________\n",
      "net_vlad_4 (NetVLAD)         (None, 16384)             32776     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16384)             65536     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               4194560   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 21)                5397      \n",
      "=================================================================\n",
      "Total params: 56,263,133\n",
      "Trainable params: 4,265,501\n",
      "Non-trainable params: 51,997,632\n",
      "_________________________________________________________________\n",
      "\n",
      "fit...\n",
      "Epoch 1/50\n",
      "281/281 [==============================] - 53s 182ms/step - loss: 0.2267 - f1macro: 0.3313 - val_loss: 0.1652 - val_f1macro: 0.0399\n",
      "Epoch 2/50\n",
      "281/281 [==============================] - 52s 184ms/step - loss: 0.1402 - f1macro: 0.5061 - val_loss: 0.0806 - val_f1macro: 0.3849\n",
      "Epoch 3/50\n",
      "281/281 [==============================] - 52s 184ms/step - loss: 0.1052 - f1macro: 0.5972 - val_loss: 0.0520 - val_f1macro: 0.6018\n",
      "Epoch 4/50\n",
      "281/281 [==============================] - 51s 182ms/step - loss: 0.0771 - f1macro: 0.6559 - val_loss: 0.0479 - val_f1macro: 0.5729\n",
      "Epoch 5/50\n",
      "281/281 [==============================] - 52s 185ms/step - loss: 0.0585 - f1macro: 0.6749 - val_loss: 0.0416 - val_f1macro: 0.5308\n",
      "Epoch 6/50\n",
      "281/281 [==============================] - 51s 182ms/step - loss: 0.0431 - f1macro: 0.6723 - val_loss: 0.0442 - val_f1macro: 0.5957\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 7/50\n",
      "281/281 [==============================] - 51s 183ms/step - loss: 0.0277 - f1macro: 0.7145 - val_loss: 0.0415 - val_f1macro: 0.5262\n",
      "Epoch 8/50\n",
      "281/281 [==============================] - 52s 186ms/step - loss: 0.0202 - f1macro: 0.7098 - val_loss: 0.0423 - val_f1macro: 0.5690\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 9/50\n",
      "281/281 [==============================] - 52s 187ms/step - loss: 0.0139 - f1macro: 0.7210 - val_loss: 0.0389 - val_f1macro: 0.5669\n",
      "Epoch 10/50\n",
      "281/281 [==============================] - 52s 184ms/step - loss: 0.0098 - f1macro: 0.7386 - val_loss: 0.0479 - val_f1macro: 0.5076\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 11/50\n",
      "281/281 [==============================] - 52s 183ms/step - loss: 0.0074 - f1macro: 0.7211 - val_loss: 0.0395 - val_f1macro: 0.5728\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "Epoch 12/50\n",
      "281/281 [==============================] - 52s 184ms/step - loss: 0.0058 - f1macro: 0.7281 - val_loss: 0.0385 - val_f1macro: 0.5659\n",
      "Epoch 13/50\n",
      "281/281 [==============================] - 53s 189ms/step - loss: 0.0048 - f1macro: 0.7336 - val_loss: 0.0399 - val_f1macro: 0.5781\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "Epoch 14/50\n",
      "281/281 [==============================] - 56s 198ms/step - loss: 0.0043 - f1macro: 0.7338 - val_loss: 0.0394 - val_f1macro: 0.5776\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "Epoch 15/50\n",
      "281/281 [==============================] - 60s 214ms/step - loss: 0.0039 - f1macro: 0.7341 - val_loss: 0.0393 - val_f1macro: 0.5958\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "Epoch 16/50\n",
      "281/281 [==============================] - 53s 190ms/step - loss: 0.0036 - f1macro: 0.7349 - val_loss: 0.0402 - val_f1macro: 0.5807\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "Epoch 17/50\n",
      "281/281 [==============================] - 57s 202ms/step - loss: 0.0035 - f1macro: 0.7393 - val_loss: 0.0405 - val_f1macro: 0.5849\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "Epoch 00017: early stopping\n",
      "predict_on_batch...\n",
      "Class weights...\n",
      "Create model...\n",
      "fine_tune_at == None\n",
      "\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_5 (KerasLayer)   (None, None, 2048)        51964864  \n",
      "_________________________________________________________________\n",
      "net_vlad_5 (NetVLAD)         (None, 16384)             32776     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16384)             65536     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               4194560   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 21)                5397      \n",
      "=================================================================\n",
      "Total params: 56,263,133\n",
      "Trainable params: 4,265,501\n",
      "Non-trainable params: 51,997,632\n",
      "_________________________________________________________________\n",
      "\n",
      "fit...\n",
      "Epoch 1/50\n",
      "281/281 [==============================] - 55s 186ms/step - loss: 0.2333 - f1macro: 0.2976 - val_loss: 0.1622 - val_f1macro: 0.0712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "281/281 [==============================] - 52s 184ms/step - loss: 0.1405 - f1macro: 0.4759 - val_loss: 0.0856 - val_f1macro: 0.3633\n",
      "Epoch 3/50\n",
      "281/281 [==============================] - 52s 185ms/step - loss: 0.1061 - f1macro: 0.5847 - val_loss: 0.0518 - val_f1macro: 0.5218\n",
      "Epoch 4/50\n",
      "281/281 [==============================] - 52s 185ms/step - loss: 0.0810 - f1macro: 0.6615 - val_loss: 0.0602 - val_f1macro: 0.4493\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 5/50\n",
      "281/281 [==============================] - 55s 195ms/step - loss: 0.0572 - f1macro: 0.6772 - val_loss: 0.0377 - val_f1macro: 0.5125\n",
      "Epoch 6/50\n",
      "281/281 [==============================] - 54s 193ms/step - loss: 0.0420 - f1macro: 0.7035 - val_loss: 0.0374 - val_f1macro: 0.6138\n",
      "Epoch 7/50\n",
      "281/281 [==============================] - 58s 205ms/step - loss: 0.0322 - f1macro: 0.7054 - val_loss: 0.0422 - val_f1macro: 0.5490\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 8/50\n",
      "281/281 [==============================] - 53s 189ms/step - loss: 0.0221 - f1macro: 0.7000 - val_loss: 0.0410 - val_f1macro: 0.5886\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 9/50\n",
      "281/281 [==============================] - 53s 190ms/step - loss: 0.0141 - f1macro: 0.7209 - val_loss: 0.0359 - val_f1macro: 0.5569\n",
      "Epoch 10/50\n",
      "281/281 [==============================] - 53s 188ms/step - loss: 0.0107 - f1macro: 0.7161 - val_loss: 0.0363 - val_f1macro: 0.5524\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "Epoch 11/50\n",
      "281/281 [==============================] - 56s 198ms/step - loss: 0.0081 - f1macro: 0.7359 - val_loss: 0.0373 - val_f1macro: 0.5709\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "Epoch 12/50\n",
      "281/281 [==============================] - 53s 190ms/step - loss: 0.0065 - f1macro: 0.7246 - val_loss: 0.0366 - val_f1macro: 0.5770\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "Epoch 13/50\n",
      "281/281 [==============================] - 56s 199ms/step - loss: 0.0054 - f1macro: 0.7383 - val_loss: 0.0367 - val_f1macro: 0.5609\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "Epoch 14/50\n",
      "281/281 [==============================] - 53s 190ms/step - loss: 0.0046 - f1macro: 0.7210 - val_loss: 0.0374 - val_f1macro: 0.5797\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "Epoch 00014: early stopping\n",
      "predict_on_batch...\n"
     ]
    }
   ],
   "source": [
    "# collect out of sample predictions\n",
    "trill_yhat = {}\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_ix, test_ix in kfold.split(X_train):\n",
    "    # get data\n",
    "    train_X, test_X = X_train.iloc[train_ix], X_train.iloc[test_ix]\n",
    "    \n",
    "    # Instanciate data generators\n",
    "    train_generator = DataGenerator_trill(train_X, **params_train)\n",
    "    test_generator = DataGenerator_trill(test_X, **params_train)\n",
    "    \n",
    "    # Class weights\n",
    "    print('Class weights...')\n",
    "    class_weights = class_weight(generator=train_generator, mu=0.675)\n",
    "    \n",
    "    # Create TRILL model\n",
    "    print('Create model...')\n",
    "    Trill = create_cnn(num_clusters=8, use_batchnorm=True,\n",
    "                       pooling=None, hidden=256,\n",
    "                       fine_tune_at=None, model_path=None)\n",
    "    Trill.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[tfa.metrics.F1Score(name='f1macro', num_classes=len(labels), average='macro')])\n",
    "    \n",
    "    print('fit...')\n",
    "    Trill.fit(\n",
    "        train_generator,\n",
    "        validation_data=test_generator,\n",
    "        epochs=50,\n",
    "        callbacks=[es_callback, reduce_lr],\n",
    "        verbose=1,\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "\n",
    "    # Predict & store\n",
    "    print('predict_on_batch...')\n",
    "    for index, row in test_generator.X.iterrows():\n",
    "        pred = Trill.predict_on_batch(data_mem[row['filename']].reshape(1, -1))\n",
    "        trill_yhat[index] = pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9610855",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "126628ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trill.save_weights(WORKING_PATH + 'trill.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "031ada7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./working/stacking/trill_yhat.jl']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(trill_yhat, WORKING_PATH + 'trill_yhat.jl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb5a627",
   "metadata": {},
   "source": [
    "## EfficientNetB0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07027773",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "095eb3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conf:\n",
    "    # Preprocessing settings\n",
    "    sampling_rate = 44100\n",
    "    n_mels = 224\n",
    "    hop_length = 494\n",
    "    n_fft = n_mels * 10\n",
    "    fmin = 20\n",
    "    fmax = 16000\n",
    "    \n",
    "    # Model parameters\n",
    "    num_rows = 224\n",
    "    num_columns = 224\n",
    "    num_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6306277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_melspectrogram(audio):\n",
    "    spectrogram = librosa.feature.melspectrogram(audio,\n",
    "                                                 sr=conf.sampling_rate,\n",
    "                                                 n_mels=conf.n_mels,\n",
    "                                                 hop_length=conf.hop_length,\n",
    "                                                 n_fft=conf.n_fft,\n",
    "                                                 fmin=conf.fmin,\n",
    "                                                 fmax=conf.fmax)\n",
    "    spectrogram = librosa.power_to_db(spectrogram)\n",
    "    spectrogram = spectrogram.astype(np.float32)\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88a00a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mono_to_color(X, eps=1e-6, mean=None, std=None):\n",
    "    \"\"\"\n",
    "    Converts a one channel array to a 3 channel one in [0, 255]\n",
    "    Arguments:\n",
    "        X {numpy array [H x W]} -- 2D array to convert\n",
    "    Keyword Arguments:\n",
    "        eps {float} -- To avoid dividing by 0 (default: {1e-6})\n",
    "        mean {None or np array} -- Mean for normalization (default: {None})\n",
    "        std {None or np array} -- Std for normalization (default: {None})\n",
    "    Returns:\n",
    "        numpy array [3 x H x W] -- RGB numpy array\n",
    "    \"\"\"\n",
    "    X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Standardize\n",
    "    mean = mean or X.mean()\n",
    "    std = std or X.std()\n",
    "    X = (X - mean) / (std + eps)\n",
    "\n",
    "    # Normalize to [0, 255]\n",
    "    _min, _max = X.min(), X.max()\n",
    "\n",
    "    if (_max - _min) > eps:\n",
    "        V = np.clip(X, _min, _max)\n",
    "        V = 255 * (V - _min) / (_max - _min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        V = np.zeros_like(X, dtype=np.uint8)\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d76890fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(y, sr):\n",
    "    # Extract features\n",
    "    feat = audio_to_melspectrogram(y)\n",
    "    feat = mono_to_color(feat)\n",
    "    feat = feat.astype(np.uint8)\n",
    "    \n",
    "    # EfficientNet preprocess\n",
    "    feat = preprocess_input(feat)\n",
    "    \n",
    "    X = np.empty((1, conf.num_rows, conf.num_columns, conf.num_channels))\n",
    "    x_features = feat.tolist()\n",
    "    X[0] = np.array(x_features)\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aac05b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    batch_size=16,\n",
    "    n_rows=conf.num_rows,\n",
    "    n_columns=conf.num_columns,\n",
    "    n_channels=conf.num_channels,\n",
    ")\n",
    "params_train = dict(\n",
    "    shuffle=False,\n",
    "    **params\n",
    ")\n",
    "params_valid = dict(\n",
    "    shuffle=False,\n",
    "    **params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f565fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370c4d4d18334bf2b3c454d706e61e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=14080)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data in RAM to speed up training process\n",
    "data_mem.clear()\n",
    "data_mem = LoadRAM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d0a3f8",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ff71f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(fine_tune_at=None,\n",
    "               model_path=None\n",
    "               ):\n",
    "\n",
    "    # Instanciate model\n",
    "    from keras.applications.efficientnet import EfficientNetB0\n",
    "    base_model = EfficientNetB0(include_top=False, input_shape=(\n",
    "        conf.num_rows, conf.num_columns, conf.num_channels), weights='imagenet', pooling='avg')\n",
    "    # Hidden neurons' number (input + output neurons) * 2/3 - 21\n",
    "    dense = Dense(142, activation='relu')(\n",
    "        base_model.output)\n",
    "    outputs = Dense(len(mlb.classes_), activation='sigmoid')(dense)\n",
    "\n",
    "    base_model.trainable = False\n",
    "\n",
    "    if fine_tune_at == None:\n",
    "        model = Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "    else:\n",
    "        model = Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "        # Load existing weights\n",
    "        model.load_weights(model_path)\n",
    "\n",
    "        # Unfreeze model layers\n",
    "        model.trainable = True\n",
    "\n",
    "        # Freeze all the layers before the `fine_tune_at` layer\n",
    "        for layer in model.layers[:fine_tune_at]:\n",
    "            layer.trainable = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22d4f4dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights...\n",
      "Create model...\n",
      "fit...\n",
      "Epoch 1/50\n",
      "563/563 [==============================] - 30s 42ms/step - loss: 0.2858 - f1macro: 0.1855 - val_loss: 0.0963 - val_f1macro: 0.2750\n",
      "Epoch 2/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.2180 - f1macro: 0.3162 - val_loss: 0.0870 - val_f1macro: 0.3035\n",
      "Epoch 3/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.1950 - f1macro: 0.3681 - val_loss: 0.0808 - val_f1macro: 0.3509\n",
      "Epoch 4/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1768 - f1macro: 0.4003 - val_loss: 0.0726 - val_f1macro: 0.3602\n",
      "Epoch 5/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1650 - f1macro: 0.4293 - val_loss: 0.0710 - val_f1macro: 0.3853\n",
      "Epoch 6/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1546 - f1macro: 0.4709 - val_loss: 0.0688 - val_f1macro: 0.3782\n",
      "Epoch 7/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1453 - f1macro: 0.4913 - val_loss: 0.0676 - val_f1macro: 0.3778\n",
      "Epoch 8/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1390 - f1macro: 0.5229 - val_loss: 0.0675 - val_f1macro: 0.4243\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 9/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1280 - f1macro: 0.5286 - val_loss: 0.0690 - val_f1macro: 0.3925\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 10/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1204 - f1macro: 0.5775 - val_loss: 0.0622 - val_f1macro: 0.3843\n",
      "Epoch 11/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1156 - f1macro: 0.5849 - val_loss: 0.0621 - val_f1macro: 0.4440\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 12/50\n",
      "563/563 [==============================] - 20s 35ms/step - loss: 0.1093 - f1macro: 0.6268 - val_loss: 0.0613 - val_f1macro: 0.4113\n",
      "Epoch 13/50\n",
      "563/563 [==============================] - 20s 35ms/step - loss: 0.1057 - f1macro: 0.6132 - val_loss: 0.0611 - val_f1macro: 0.3981\n",
      "Epoch 14/50\n",
      "563/563 [==============================] - 20s 35ms/step - loss: 0.1023 - f1macro: 0.6216 - val_loss: 0.0605 - val_f1macro: 0.4419\n",
      "Epoch 15/50\n",
      "563/563 [==============================] - 22s 39ms/step - loss: 0.1020 - f1macro: 0.6137 - val_loss: 0.0576 - val_f1macro: 0.4522\n",
      "Epoch 16/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.0973 - f1macro: 0.6297 - val_loss: 0.0580 - val_f1macro: 0.4570\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "Epoch 17/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0929 - f1macro: 0.6135 - val_loss: 0.0578 - val_f1macro: 0.4572\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "Epoch 18/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0910 - f1macro: 0.6451 - val_loss: 0.0569 - val_f1macro: 0.4790\n",
      "Epoch 19/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0884 - f1macro: 0.6567 - val_loss: 0.0585 - val_f1macro: 0.4477\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "Epoch 20/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0850 - f1macro: 0.6630 - val_loss: 0.0567 - val_f1macro: 0.4598\n",
      "Epoch 21/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0849 - f1macro: 0.6618 - val_loss: 0.0575 - val_f1macro: 0.4794\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "Epoch 22/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0814 - f1macro: 0.6778 - val_loss: 0.0561 - val_f1macro: 0.4585\n",
      "Epoch 23/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0817 - f1macro: 0.6681 - val_loss: 0.0582 - val_f1macro: 0.4552\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "Epoch 24/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0789 - f1macro: 0.6646 - val_loss: 0.0567 - val_f1macro: 0.4607\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "Epoch 25/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0778 - f1macro: 0.6698 - val_loss: 0.0571 - val_f1macro: 0.4614\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
      "Epoch 26/50\n",
      "563/563 [==============================] - 21s 36ms/step - loss: 0.0774 - f1macro: 0.6803 - val_loss: 0.0563 - val_f1macro: 0.4580\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 8.589935605414213e-05.\n",
      "Epoch 27/50\n",
      "563/563 [==============================] - 20s 35ms/step - loss: 0.0758 - f1macro: 0.6771 - val_loss: 0.0560 - val_f1macro: 0.4789\n",
      "Epoch 28/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0749 - f1macro: 0.6701 - val_loss: 0.0559 - val_f1macro: 0.4907\n",
      "Epoch 29/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0761 - f1macro: 0.6796 - val_loss: 0.0559 - val_f1macro: 0.4621\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 6.871948717162013e-05.\n",
      "Epoch 30/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0747 - f1macro: 0.6837 - val_loss: 0.0553 - val_f1macro: 0.4883\n",
      "Epoch 31/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.0748 - f1macro: 0.6818 - val_loss: 0.0559 - val_f1macro: 0.5026\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.497558740898967e-05.\n",
      "Epoch 32/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0742 - f1macro: 0.6896 - val_loss: 0.0565 - val_f1macro: 0.4851\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 4.398046876303852e-05.\n",
      "Epoch 33/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0722 - f1macro: 0.6797 - val_loss: 0.0556 - val_f1macro: 0.4889\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.518437442835421e-05.\n",
      "Epoch 34/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0723 - f1macro: 0.6849 - val_loss: 0.0561 - val_f1macro: 0.4845\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 2.8147498960606756e-05.\n",
      "Epoch 35/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0729 - f1macro: 0.6900 - val_loss: 0.0555 - val_f1macro: 0.4845\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 2.25179988774471e-05.\n",
      "Epoch 00035: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60f26f3f42a488d86449226c3ddca35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=2253)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_on_batch...\n",
      "Class weights...\n",
      "Create model...\n",
      "fit...\n",
      "Epoch 1/50\n",
      "563/563 [==============================] - 25s 37ms/step - loss: 0.2826 - f1macro: 0.1854 - val_loss: 0.1013 - val_f1macro: 0.2879\n",
      "Epoch 2/50\n",
      "563/563 [==============================] - 20s 35ms/step - loss: 0.2154 - f1macro: 0.3044 - val_loss: 0.0907 - val_f1macro: 0.2919\n",
      "Epoch 3/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1904 - f1macro: 0.3658 - val_loss: 0.0826 - val_f1macro: 0.3706\n",
      "Epoch 4/50\n",
      "563/563 [==============================] - 20s 35ms/step - loss: 0.1757 - f1macro: 0.3894 - val_loss: 0.0788 - val_f1macro: 0.3513\n",
      "Epoch 5/50\n",
      "563/563 [==============================] - 20s 35ms/step - loss: 0.1633 - f1macro: 0.4289 - val_loss: 0.0752 - val_f1macro: 0.4438\n",
      "Epoch 6/50\n",
      "563/563 [==============================] - 20s 35ms/step - loss: 0.1539 - f1macro: 0.4673 - val_loss: 0.0717 - val_f1macro: 0.4493\n",
      "Epoch 7/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1456 - f1macro: 0.5210 - val_loss: 0.0709 - val_f1macro: 0.4243\n",
      "Epoch 8/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1376 - f1macro: 0.5432 - val_loss: 0.0733 - val_f1macro: 0.4387\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 9/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1275 - f1macro: 0.5711 - val_loss: 0.0687 - val_f1macro: 0.4917\n",
      "Epoch 10/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1223 - f1macro: 0.5885 - val_loss: 0.0672 - val_f1macro: 0.4583\n",
      "Epoch 11/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1173 - f1macro: 0.5930 - val_loss: 0.0632 - val_f1macro: 0.5243\n",
      "Epoch 12/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1127 - f1macro: 0.6082 - val_loss: 0.0690 - val_f1macro: 0.4815\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 13/50\n",
      "563/563 [==============================] - 20s 35ms/step - loss: 0.1057 - f1macro: 0.6382 - val_loss: 0.0628 - val_f1macro: 0.4991\n",
      "Epoch 14/50\n",
      "563/563 [==============================] - 20s 35ms/step - loss: 0.1011 - f1macro: 0.6404 - val_loss: 0.0619 - val_f1macro: 0.5000\n",
      "Epoch 15/50\n",
      "563/563 [==============================] - 20s 35ms/step - loss: 0.0993 - f1macro: 0.6310 - val_loss: 0.0635 - val_f1macro: 0.5094\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 16/50\n",
      "563/563 [==============================] - 23s 40ms/step - loss: 0.0946 - f1macro: 0.6458 - val_loss: 0.0620 - val_f1macro: 0.5338\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "Epoch 17/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.0907 - f1macro: 0.6477 - val_loss: 0.0605 - val_f1macro: 0.5573\n",
      "Epoch 18/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0877 - f1macro: 0.6597 - val_loss: 0.0597 - val_f1macro: 0.5119\n",
      "Epoch 19/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0853 - f1macro: 0.6590 - val_loss: 0.0585 - val_f1macro: 0.5459\n",
      "Epoch 20/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0842 - f1macro: 0.6753 - val_loss: 0.0597 - val_f1macro: 0.5213\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "Epoch 21/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0805 - f1macro: 0.6697 - val_loss: 0.0604 - val_f1macro: 0.5356\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "Epoch 22/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0784 - f1macro: 0.6876 - val_loss: 0.0597 - val_f1macro: 0.5529\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "Epoch 23/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0765 - f1macro: 0.6852 - val_loss: 0.0580 - val_f1macro: 0.5469\n",
      "Epoch 24/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0758 - f1macro: 0.6887 - val_loss: 0.0588 - val_f1macro: 0.5022\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "Epoch 25/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0739 - f1macro: 0.6852 - val_loss: 0.0580 - val_f1macro: 0.5446\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "Epoch 26/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0714 - f1macro: 0.6833 - val_loss: 0.0590 - val_f1macro: 0.5614\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
      "Epoch 27/50\n",
      "563/563 [==============================] - 20s 35ms/step - loss: 0.0719 - f1macro: 0.7063 - val_loss: 0.0579 - val_f1macro: 0.5456\n",
      "Epoch 28/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0711 - f1macro: 0.6908 - val_loss: 0.0578 - val_f1macro: 0.5657\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 8.589935605414213e-05.\n",
      "Epoch 29/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0697 - f1macro: 0.6941 - val_loss: 0.0574 - val_f1macro: 0.5680\n",
      "Epoch 30/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.0693 - f1macro: 0.6950 - val_loss: 0.0583 - val_f1macro: 0.5561\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.871948717162013e-05.\n",
      "Epoch 31/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0700 - f1macro: 0.6989 - val_loss: 0.0579 - val_f1macro: 0.5623\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.497558740898967e-05.\n",
      "Epoch 32/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0680 - f1macro: 0.6961 - val_loss: 0.0577 - val_f1macro: 0.5468\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 4.398046876303852e-05.\n",
      "Epoch 33/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0672 - f1macro: 0.7059 - val_loss: 0.0577 - val_f1macro: 0.5503\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.518437442835421e-05.\n",
      "Epoch 34/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0673 - f1macro: 0.7019 - val_loss: 0.0575 - val_f1macro: 0.5597\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 2.8147498960606756e-05.\n",
      "Epoch 00034: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9084d41353b48b5942d3ba226c20e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=2253)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_on_batch...\n",
      "Class weights...\n",
      "Create model...\n",
      "fit...\n",
      "Epoch 1/50\n",
      "563/563 [==============================] - 29s 43ms/step - loss: 0.2851 - f1macro: 0.1976 - val_loss: 0.1021 - val_f1macro: 0.2687\n",
      "Epoch 2/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.2177 - f1macro: 0.3022 - val_loss: 0.0855 - val_f1macro: 0.3198\n",
      "Epoch 3/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.1932 - f1macro: 0.3452 - val_loss: 0.0832 - val_f1macro: 0.3691\n",
      "Epoch 4/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.1772 - f1macro: 0.3961 - val_loss: 0.0748 - val_f1macro: 0.4584\n",
      "Epoch 5/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.1660 - f1macro: 0.4611 - val_loss: 0.0732 - val_f1macro: 0.4043\n",
      "Epoch 6/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.1554 - f1macro: 0.4810 - val_loss: 0.0757 - val_f1macro: 0.3767\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 7/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.1457 - f1macro: 0.5012 - val_loss: 0.0693 - val_f1macro: 0.4856\n",
      "Epoch 8/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.1381 - f1macro: 0.5492 - val_loss: 0.0652 - val_f1macro: 0.4293\n",
      "Epoch 9/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.1325 - f1macro: 0.5508 - val_loss: 0.0648 - val_f1macro: 0.5157\n",
      "Epoch 10/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.1274 - f1macro: 0.5402 - val_loss: 0.0660 - val_f1macro: 0.5049\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 11/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.1202 - f1macro: 0.5740 - val_loss: 0.0598 - val_f1macro: 0.4620\n",
      "Epoch 12/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.1149 - f1macro: 0.6082 - val_loss: 0.0605 - val_f1macro: 0.4315\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 13/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.1098 - f1macro: 0.6036 - val_loss: 0.0588 - val_f1macro: 0.4729\n",
      "Epoch 14/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.1089 - f1macro: 0.6223 - val_loss: 0.0633 - val_f1macro: 0.4766\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "Epoch 15/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.1027 - f1macro: 0.6373 - val_loss: 0.0594 - val_f1macro: 0.4951\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "Epoch 16/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0992 - f1macro: 0.6507 - val_loss: 0.0595 - val_f1macro: 0.5059\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "Epoch 17/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0947 - f1macro: 0.6416 - val_loss: 0.0582 - val_f1macro: 0.4787\n",
      "Epoch 18/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0948 - f1macro: 0.6491 - val_loss: 0.0558 - val_f1macro: 0.4587\n",
      "Epoch 19/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0935 - f1macro: 0.6337 - val_loss: 0.0562 - val_f1macro: 0.4934\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "Epoch 20/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0908 - f1macro: 0.6488 - val_loss: 0.0572 - val_f1macro: 0.5001\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "Epoch 21/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0893 - f1macro: 0.6549 - val_loss: 0.0558 - val_f1macro: 0.5234\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "Epoch 22/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0895 - f1macro: 0.6547 - val_loss: 0.0561 - val_f1macro: 0.4766\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
      "Epoch 23/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0871 - f1macro: 0.6553 - val_loss: 0.0554 - val_f1macro: 0.5254\n",
      "Epoch 24/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0857 - f1macro: 0.6433 - val_loss: 0.0550 - val_f1macro: 0.4867\n",
      "Epoch 25/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0852 - f1macro: 0.6597 - val_loss: 0.0549 - val_f1macro: 0.5208\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 8.589935605414213e-05.\n",
      "Epoch 26/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0843 - f1macro: 0.6602 - val_loss: 0.0560 - val_f1macro: 0.5246\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 6.871948717162013e-05.\n",
      "Epoch 27/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0837 - f1macro: 0.6717 - val_loss: 0.0553 - val_f1macro: 0.4756\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 5.497558740898967e-05.\n",
      "Epoch 28/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.0830 - f1macro: 0.6553 - val_loss: 0.0550 - val_f1macro: 0.4720\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 4.398046876303852e-05.\n",
      "Epoch 29/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0818 - f1macro: 0.6702 - val_loss: 0.0547 - val_f1macro: 0.4751\n",
      "Epoch 30/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0815 - f1macro: 0.6785 - val_loss: 0.0551 - val_f1macro: 0.5278\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 3.518437442835421e-05.\n",
      "Epoch 31/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0828 - f1macro: 0.6745 - val_loss: 0.0549 - val_f1macro: 0.4787\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 2.8147498960606756e-05.\n",
      "Epoch 32/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.0809 - f1macro: 0.6722 - val_loss: 0.0547 - val_f1macro: 0.5285\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 2.25179988774471e-05.\n",
      "Epoch 33/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.0817 - f1macro: 0.6654 - val_loss: 0.0547 - val_f1macro: 0.5318\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.8014399392995985e-05.\n",
      "Epoch 34/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.0801 - f1macro: 0.6567 - val_loss: 0.0548 - val_f1macro: 0.5239\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.4411519805435093e-05.\n",
      "Epoch 35/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.0801 - f1macro: 0.6660 - val_loss: 0.0546 - val_f1macro: 0.5245\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.1529216135386379e-05.\n",
      "Epoch 36/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.0807 - f1macro: 0.6588 - val_loss: 0.0547 - val_f1macro: 0.5250\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 37/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.0810 - f1macro: 0.6850 - val_loss: 0.0548 - val_f1macro: 0.5236\n",
      "Epoch 38/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0800 - f1macro: 0.6728 - val_loss: 0.0546 - val_f1macro: 0.5222\n",
      "Epoch 39/50\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.0799 - f1macro: 0.6692 - val_loss: 0.0547 - val_f1macro: 0.5273\n",
      "Epoch 40/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.0798 - f1macro: 0.6818 - val_loss: 0.0546 - val_f1macro: 0.5268\n",
      "Epoch 41/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.0786 - f1macro: 0.6904 - val_loss: 0.0547 - val_f1macro: 0.4729\n",
      "Epoch 42/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.0799 - f1macro: 0.6752 - val_loss: 0.0546 - val_f1macro: 0.5254\n",
      "Epoch 43/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.0794 - f1macro: 0.6584 - val_loss: 0.0546 - val_f1macro: 0.5227\n",
      "Epoch 44/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.0793 - f1macro: 0.6708 - val_loss: 0.0546 - val_f1macro: 0.4773\n",
      "Epoch 45/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.0804 - f1macro: 0.6720 - val_loss: 0.0545 - val_f1macro: 0.4780\n",
      "Epoch 46/50\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.0803 - f1macro: 0.6730 - val_loss: 0.0546 - val_f1macro: 0.4755\n",
      "Epoch 47/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0801 - f1macro: 0.6740 - val_loss: 0.0547 - val_f1macro: 0.5222\n",
      "Epoch 48/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0799 - f1macro: 0.6737 - val_loss: 0.0545 - val_f1macro: 0.5244\n",
      "Epoch 49/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0797 - f1macro: 0.6811 - val_loss: 0.0546 - val_f1macro: 0.5251\n",
      "Epoch 50/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0808 - f1macro: 0.6514 - val_loss: 0.0546 - val_f1macro: 0.5305\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00050: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec9acf1bf4d44efb319271daff76506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=2253)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_on_batch...\n",
      "Class weights...\n",
      "Create model...\n",
      "fit...\n",
      "Epoch 1/50\n",
      "563/563 [==============================] - 29s 44ms/step - loss: 0.2782 - f1macro: 0.1905 - val_loss: 0.1006 - val_f1macro: 0.2224\n",
      "Epoch 2/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.2126 - f1macro: 0.3117 - val_loss: 0.0874 - val_f1macro: 0.3365\n",
      "Epoch 3/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.1918 - f1macro: 0.3628 - val_loss: 0.0827 - val_f1macro: 0.3668\n",
      "Epoch 4/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.1745 - f1macro: 0.4279 - val_loss: 0.0751 - val_f1macro: 0.3714\n",
      "Epoch 5/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.1622 - f1macro: 0.4388 - val_loss: 0.0768 - val_f1macro: 0.4255\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 6/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.1509 - f1macro: 0.4882 - val_loss: 0.0697 - val_f1macro: 0.3771\n",
      "Epoch 7/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.1431 - f1macro: 0.5154 - val_loss: 0.0687 - val_f1macro: 0.4367\n",
      "Epoch 8/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.1353 - f1macro: 0.5394 - val_loss: 0.0667 - val_f1macro: 0.4278\n",
      "Epoch 9/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.1299 - f1macro: 0.5460 - val_loss: 0.0660 - val_f1macro: 0.4395\n",
      "Epoch 10/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.1261 - f1macro: 0.5425 - val_loss: 0.0656 - val_f1macro: 0.4465\n",
      "Epoch 11/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.1205 - f1macro: 0.5796 - val_loss: 0.0650 - val_f1macro: 0.4471\n",
      "Epoch 12/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.1155 - f1macro: 0.5900 - val_loss: 0.0622 - val_f1macro: 0.5000\n",
      "Epoch 13/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.1114 - f1macro: 0.6007 - val_loss: 0.0619 - val_f1macro: 0.4619\n",
      "Epoch 14/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.1076 - f1macro: 0.6039 - val_loss: 0.0618 - val_f1macro: 0.4642\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 15/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.1011 - f1macro: 0.6113 - val_loss: 0.0613 - val_f1macro: 0.4945\n",
      "Epoch 16/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.1002 - f1macro: 0.6261 - val_loss: 0.0585 - val_f1macro: 0.4888\n",
      "Epoch 17/50\n",
      "563/563 [==============================] - 24s 43ms/step - loss: 0.0950 - f1macro: 0.6328 - val_loss: 0.0611 - val_f1macro: 0.4875\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 18/50\n",
      "563/563 [==============================] - 24s 43ms/step - loss: 0.0887 - f1macro: 0.6446 - val_loss: 0.0615 - val_f1macro: 0.4282\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "Epoch 19/50\n",
      "563/563 [==============================] - 24s 43ms/step - loss: 0.0861 - f1macro: 0.6596 - val_loss: 0.0594 - val_f1macro: 0.4794\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "Epoch 20/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0833 - f1macro: 0.6611 - val_loss: 0.0589 - val_f1macro: 0.4768\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "Epoch 21/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0803 - f1macro: 0.6690 - val_loss: 0.0576 - val_f1macro: 0.4806\n",
      "Epoch 22/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0808 - f1macro: 0.6567 - val_loss: 0.0578 - val_f1macro: 0.5082\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "Epoch 23/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0780 - f1macro: 0.6749 - val_loss: 0.0576 - val_f1macro: 0.4978\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "Epoch 24/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0766 - f1macro: 0.6896 - val_loss: 0.0567 - val_f1macro: 0.5108\n",
      "Epoch 25/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0751 - f1macro: 0.6845 - val_loss: 0.0565 - val_f1macro: 0.4725\n",
      "Epoch 26/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0753 - f1macro: 0.6937 - val_loss: 0.0564 - val_f1macro: 0.5056\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "Epoch 27/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0739 - f1macro: 0.7014 - val_loss: 0.0575 - val_f1macro: 0.5097\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
      "Epoch 28/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0725 - f1macro: 0.6882 - val_loss: 0.0560 - val_f1macro: 0.5010\n",
      "Epoch 29/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0721 - f1macro: 0.6903 - val_loss: 0.0561 - val_f1macro: 0.4979\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 8.589935605414213e-05.\n",
      "Epoch 30/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0710 - f1macro: 0.6920 - val_loss: 0.0560 - val_f1macro: 0.5004\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.871948717162013e-05.\n",
      "Epoch 31/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0703 - f1macro: 0.6946 - val_loss: 0.0562 - val_f1macro: 0.4944\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.497558740898967e-05.\n",
      "Epoch 32/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0707 - f1macro: 0.6948 - val_loss: 0.0561 - val_f1macro: 0.4964\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 4.398046876303852e-05.\n",
      "Epoch 33/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0709 - f1macro: 0.6870 - val_loss: 0.0559 - val_f1macro: 0.4977\n",
      "Epoch 34/50\n",
      "563/563 [==============================] - 24s 43ms/step - loss: 0.0692 - f1macro: 0.7044 - val_loss: 0.0559 - val_f1macro: 0.5008\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 3.518437442835421e-05.\n",
      "Epoch 35/50\n",
      "563/563 [==============================] - 24s 43ms/step - loss: 0.0709 - f1macro: 0.7033 - val_loss: 0.0561 - val_f1macro: 0.4973\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 2.8147498960606756e-05.\n",
      "Epoch 36/50\n",
      "563/563 [==============================] - 24s 43ms/step - loss: 0.0687 - f1macro: 0.6944 - val_loss: 0.0559 - val_f1macro: 0.4991\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 2.25179988774471e-05.\n",
      "Epoch 37/50\n",
      "563/563 [==============================] - 24s 43ms/step - loss: 0.0685 - f1macro: 0.6806 - val_loss: 0.0564 - val_f1macro: 0.4998\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.8014399392995985e-05.\n",
      "Epoch 38/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0688 - f1macro: 0.7036 - val_loss: 0.0558 - val_f1macro: 0.5028\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.4411519805435093e-05.\n",
      "Epoch 39/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0685 - f1macro: 0.7025 - val_loss: 0.0559 - val_f1macro: 0.5008\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.1529216135386379e-05.\n",
      "Epoch 40/50\n",
      "563/563 [==============================] - 24s 43ms/step - loss: 0.0683 - f1macro: 0.7079 - val_loss: 0.0558 - val_f1macro: 0.5007\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 41/50\n",
      "563/563 [==============================] - 24s 43ms/step - loss: 0.0675 - f1macro: 0.7037 - val_loss: 0.0559 - val_f1macro: 0.5008\n",
      "Epoch 42/50\n",
      "563/563 [==============================] - 24s 43ms/step - loss: 0.0687 - f1macro: 0.7011 - val_loss: 0.0560 - val_f1macro: 0.4999\n",
      "Epoch 43/50\n",
      "563/563 [==============================] - 24s 43ms/step - loss: 0.0678 - f1macro: 0.7074 - val_loss: 0.0559 - val_f1macro: 0.5001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00043: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40feb803324043b08ff501b5d5785e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=2253)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_on_batch...\n",
      "Class weights...\n",
      "Create model...\n",
      "fit...\n",
      "Epoch 1/50\n",
      "563/563 [==============================] - 26s 37ms/step - loss: 0.2831 - f1macro: 0.1952 - val_loss: 0.1002 - val_f1macro: 0.2590\n",
      "Epoch 2/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.2164 - f1macro: 0.3170 - val_loss: 0.0869 - val_f1macro: 0.2968\n",
      "Epoch 3/50\n",
      "563/563 [==============================] - 20s 35ms/step - loss: 0.1935 - f1macro: 0.3856 - val_loss: 0.0816 - val_f1macro: 0.3384\n",
      "Epoch 4/50\n",
      "563/563 [==============================] - 20s 35ms/step - loss: 0.1766 - f1macro: 0.4014 - val_loss: 0.0768 - val_f1macro: 0.4096\n",
      "Epoch 5/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1658 - f1macro: 0.4658 - val_loss: 0.0710 - val_f1macro: 0.3810\n",
      "Epoch 6/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1558 - f1macro: 0.4630 - val_loss: 0.0753 - val_f1macro: 0.4865\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 7/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1441 - f1macro: 0.5224 - val_loss: 0.0650 - val_f1macro: 0.4894\n",
      "Epoch 8/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1370 - f1macro: 0.5219 - val_loss: 0.0663 - val_f1macro: 0.4294\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 9/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1288 - f1macro: 0.5596 - val_loss: 0.0633 - val_f1macro: 0.4399\n",
      "Epoch 10/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1240 - f1macro: 0.5724 - val_loss: 0.0630 - val_f1macro: 0.4573\n",
      "Epoch 11/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1215 - f1macro: 0.5812 - val_loss: 0.0648 - val_f1macro: 0.4617\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 12/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1137 - f1macro: 0.6052 - val_loss: 0.0605 - val_f1macro: 0.4608\n",
      "Epoch 13/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1111 - f1macro: 0.6009 - val_loss: 0.0588 - val_f1macro: 0.5138\n",
      "Epoch 14/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1080 - f1macro: 0.6204 - val_loss: 0.0585 - val_f1macro: 0.4567\n",
      "Epoch 15/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1045 - f1macro: 0.6097 - val_loss: 0.0598 - val_f1macro: 0.4768\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "Epoch 16/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.1019 - f1macro: 0.6239 - val_loss: 0.0573 - val_f1macro: 0.5044\n",
      "Epoch 17/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0987 - f1macro: 0.6328 - val_loss: 0.0563 - val_f1macro: 0.4610\n",
      "Epoch 18/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0973 - f1macro: 0.6426 - val_loss: 0.0585 - val_f1macro: 0.4887\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "Epoch 19/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0936 - f1macro: 0.6373 - val_loss: 0.0562 - val_f1macro: 0.4962\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "Epoch 20/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0897 - f1macro: 0.6480 - val_loss: 0.0559 - val_f1macro: 0.5013\n",
      "Epoch 21/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0899 - f1macro: 0.6522 - val_loss: 0.0553 - val_f1macro: 0.4830\n",
      "Epoch 22/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0885 - f1macro: 0.6482 - val_loss: 0.0559 - val_f1macro: 0.4758\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "Epoch 23/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0854 - f1macro: 0.6628 - val_loss: 0.0556 - val_f1macro: 0.5103\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "Epoch 24/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0831 - f1macro: 0.6742 - val_loss: 0.0560 - val_f1macro: 0.5550\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "Epoch 25/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0825 - f1macro: 0.6577 - val_loss: 0.0555 - val_f1macro: 0.5143\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
      "Epoch 26/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0818 - f1macro: 0.6601 - val_loss: 0.0549 - val_f1macro: 0.5123\n",
      "Epoch 27/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0811 - f1macro: 0.6720 - val_loss: 0.0547 - val_f1macro: 0.5126\n",
      "Epoch 28/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0801 - f1macro: 0.6739 - val_loss: 0.0550 - val_f1macro: 0.5022\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 8.589935605414213e-05.\n",
      "Epoch 29/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0790 - f1macro: 0.6600 - val_loss: 0.0549 - val_f1macro: 0.5092\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 6.871948717162013e-05.\n",
      "Epoch 30/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0785 - f1macro: 0.6792 - val_loss: 0.0546 - val_f1macro: 0.4974\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.497558740898967e-05.\n",
      "Epoch 31/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0788 - f1macro: 0.6670 - val_loss: 0.0547 - val_f1macro: 0.5037\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 4.398046876303852e-05.\n",
      "Epoch 32/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0775 - f1macro: 0.6749 - val_loss: 0.0546 - val_f1macro: 0.5042\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 3.518437442835421e-05.\n",
      "Epoch 33/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0779 - f1macro: 0.6859 - val_loss: 0.0543 - val_f1macro: 0.5046\n",
      "Epoch 34/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0772 - f1macro: 0.6758 - val_loss: 0.0543 - val_f1macro: 0.5050\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 2.8147498960606756e-05.\n",
      "Epoch 35/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0765 - f1macro: 0.6652 - val_loss: 0.0543 - val_f1macro: 0.5031\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 2.25179988774471e-05.\n",
      "Epoch 36/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0769 - f1macro: 0.6625 - val_loss: 0.0541 - val_f1macro: 0.5037\n",
      "Epoch 37/50\n",
      "563/563 [==============================] - 20s 36ms/step - loss: 0.0763 - f1macro: 0.6843 - val_loss: 0.0541 - val_f1macro: 0.4962\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.8014399392995985e-05.\n",
      "Epoch 38/50\n",
      "563/563 [==============================] - 21s 38ms/step - loss: 0.0766 - f1macro: 0.6669 - val_loss: 0.0543 - val_f1macro: 0.5047\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.4411519805435093e-05.\n",
      "Epoch 39/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0761 - f1macro: 0.6721 - val_loss: 0.0543 - val_f1macro: 0.5055\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.1529216135386379e-05.\n",
      "Epoch 40/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0763 - f1macro: 0.6760 - val_loss: 0.0542 - val_f1macro: 0.5029\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 41/50\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.0759 - f1macro: 0.6721 - val_loss: 0.0542 - val_f1macro: 0.5056\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00041: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15fb4d6086844c348c525b5fa053bdd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=2252)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_on_batch...\n"
     ]
    }
   ],
   "source": [
    "# collect out of sample predictions\n",
    "EfficientNetB0_yhat = {}\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_ix, test_ix in kfold.split(X_train):\n",
    "    # get data\n",
    "    train_X, test_X = X_train.iloc[train_ix], X_train.iloc[test_ix]\n",
    "\n",
    "    # Instanciate data generators\n",
    "    train_generator = DataGenerator_EfficientNetB0(train_X, **params_train)\n",
    "    test_generator = DataGenerator_EfficientNetB0(test_X, **params_train)\n",
    "    \n",
    "    # Class weights\n",
    "    print('Class weights...')\n",
    "    class_weights = class_weight(generator=train_generator, mu=0.675)\n",
    "\n",
    "    # Create EfficientNetB0 model\n",
    "    print('Create model...')\n",
    "    EfficientNetB0 = create_cnn()\n",
    "    EfficientNetB0.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                     loss='binary_crossentropy',\n",
    "                     metrics=[tfa.metrics.F1Score(name='f1macro', num_classes=len(labels), average='macro')])\n",
    "\n",
    "    print('fit...')\n",
    "    EfficientNetB0.fit(\n",
    "        train_generator,\n",
    "        validation_data=test_generator,\n",
    "        epochs=50,\n",
    "        callbacks=[es_callback, reduce_lr],\n",
    "        verbose=1,\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "\n",
    "    # Predict & store\n",
    "    # Instantiate the progress bar\n",
    "    max_count = test_generator.X.shape[0]\n",
    "    f = IntProgress(min=0, max=max_count)\n",
    "    # Display the progress bar\n",
    "    display(f)\n",
    "\n",
    "    print('predict_on_batch...')\n",
    "    for index, row in test_generator.X.iterrows():\n",
    "        # Increment the progress bar\n",
    "        f.value += 1\n",
    "        # Format data\n",
    "        X = np.empty((1, conf.num_rows, conf.num_columns, conf.num_channels))\n",
    "        X[0] = np.array(data_mem[row['filename']])\n",
    "        # Predict\n",
    "        pred = EfficientNetB0.predict_on_batch(X)\n",
    "        # Store prediction\n",
    "        EfficientNetB0_yhat[index] = pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f3292",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbe21ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EfficientNetB0.save_weights(WORKING_PATH + 'EfficientNetB0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3622edc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./working/stacking/EfficientNetB0_yhat.jl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(EfficientNetB0_yhat, WORKING_PATH + 'EfficientNetB0_yhat.jl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49871563",
   "metadata": {},
   "source": [
    "## VGGish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3596d0",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b88ad064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sound noise reduction\n",
    "def f_high(y,sr):\n",
    "    b,a = signal.butter(10, 2000/(sr/2), btype='highpass')\n",
    "    yf = signal.lfilter(b,a,y)\n",
    "    return yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad35f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(y, sr):\n",
    "    # Sound noise reduction\n",
    "    y = f_high(y, sr)\n",
    "    \n",
    "    feat = vggish_input.waveform_to_examples(y, sr)\n",
    "        \n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e09345cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    batch_size=32,\n",
    "    n_rows=5,\n",
    "    n_columns=96,\n",
    "    n_channels=64,\n",
    ")\n",
    "params_train = dict(\n",
    "    shuffle=True,\n",
    "    **params\n",
    ")\n",
    "params_valid = dict(\n",
    "    shuffle=False,\n",
    "    **params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0596f656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ebf4f1014844dcad6f1ede8ccf8927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=14080)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data in RAM to speed up training process\n",
    "data_mem.clear()\n",
    "data_mem = LoadRAM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68f1762",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e137df09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(fine_tune_at=None,\n",
    "               model_path=None\n",
    "               ):\n",
    "\n",
    "    # Instanciate model\n",
    "    base_model, _, _ = vgk.get_embedding_model(hop_duration=0.25)   \n",
    "    dense = Dense(128, activation='relu')(base_model.output)\n",
    "    outputs = Dense(len(mlb.classes_), activation='sigmoid')(dense)\n",
    "      \n",
    "    base_model.trainable = True\n",
    "    \n",
    "    if fine_tune_at == None:     \n",
    "        model = Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "    else:\n",
    "        model = Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "        # Load existing weights\n",
    "        model.load_weights(model_path)\n",
    "\n",
    "        # Unfreeze model layers\n",
    "        model.trainable = True\n",
    "\n",
    "        # Freeze all the layers before the `fine_tune_at` layer\n",
    "        for layer in model.layers[:fine_tune_at]:\n",
    "            layer.trainable = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f24558c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights...\n",
      "Create model...\n",
      "fit...\n",
      "Epoch 1/50\n",
      "281/281 [==============================] - 24s 72ms/step - loss: 0.3944 - f1macro: 0.0415 - val_loss: 0.1796 - val_f1macro: 0.0275\n",
      "Epoch 2/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.3673 - f1macro: 0.0476 - val_loss: 0.1481 - val_f1macro: 0.0633\n",
      "Epoch 3/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.3409 - f1macro: 0.0753 - val_loss: 0.1575 - val_f1macro: 0.0608\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 4/50\n",
      "281/281 [==============================] - 22s 77ms/step - loss: 0.3191 - f1macro: 0.0858 - val_loss: 0.1256 - val_f1macro: 0.0945\n",
      "Epoch 5/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.2944 - f1macro: 0.1190 - val_loss: 0.1274 - val_f1macro: 0.1341\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 6/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.2704 - f1macro: 0.1482 - val_loss: 0.1097 - val_f1macro: 0.1411\n",
      "Epoch 7/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.2539 - f1macro: 0.1719 - val_loss: 0.1108 - val_f1macro: 0.1684oss: 0.2530 - f1 - ETA: 1s - loss:\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 8/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.2370 - f1macro: 0.1920 - val_loss: 0.1060 - val_f1macro: 0.1981\n",
      "Epoch 9/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.2312 - f1macro: 0.2067 - val_loss: 0.0953 - val_f1macro: 0.2058\n",
      "Epoch 10/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.2180 - f1macro: 0.2160 - val_loss: 0.0964 - val_f1macro: 0.2040\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "Epoch 11/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.2033 - f1macro: 0.2312 - val_loss: 0.0925 - val_f1macro: 0.2344\n",
      "Epoch 12/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1925 - f1macro: 0.2505 - val_loss: 0.1096 - val_f1macro: 0.2062\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "Epoch 13/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.1820 - f1macro: 0.2879 - val_loss: 0.0870 - val_f1macro: 0.2574\n",
      "Epoch 14/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.1717 - f1macro: 0.3031 - val_loss: 0.0884 - val_f1macro: 0.2915\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "Epoch 15/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1607 - f1macro: 0.3257 - val_loss: 0.0916 - val_f1macro: 0.2852\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "Epoch 16/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.1495 - f1macro: 0.3398 - val_loss: 0.0858 - val_f1macro: 0.2958\n",
      "Epoch 17/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.1421 - f1macro: 0.3636 - val_loss: 0.0881 - val_f1macro: 0.3095\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "Epoch 18/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.1315 - f1macro: 0.4006 - val_loss: 0.0890 - val_f1macro: 0.3157\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "Epoch 19/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.1232 - f1macro: 0.4456 - val_loss: 0.0887 - val_f1macro: 0.3439\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
      "Epoch 20/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1158 - f1macro: 0.4718 - val_loss: 0.0914 - val_f1macro: 0.3305ro:  - ETA: 3s - loss: 0.1161 - f1macro: 0. - ETA: 2s - loss: 0.1154  - ETA: 1s - loss: 0.1153 - f1macro: 0.469 - ETA: 1s - loss: 0.1156 - f1 - ETA: 1s - loss: 0.115\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 8.589935605414213e-05.\n",
      "Epoch 21/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.1099 - f1macro: 0.4852 - val_loss: 0.0897 - val_f1macro: 0.3585: 12s - lo - ETA: 11s - loss: 0.1094 - - ETA: 10s  - ET\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.871948717162013e-05.\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd0fa23cbcd42efb79ae0c5270c1853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=2253)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_on_batch...\n",
      "Class weights...\n",
      "Create model...\n",
      "fit...\n",
      "Epoch 1/50\n",
      "281/281 [==============================] - 22s 74ms/step - loss: 0.3916 - f1macro: 0.0397 - val_loss: 0.1760 - val_f1macro: 0.0271\n",
      "Epoch 2/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.3597 - f1macro: 0.0501 - val_loss: 0.1579 - val_f1macro: 0.0636.3617 - f1m -\n",
      "Epoch 3/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.3255 - f1macro: 0.0890 - val_loss: 0.1550 - val_f1macro: 0.0664\n",
      "Epoch 4/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.3036 - f1macro: 0.1113 - val_loss: 0.1282 - val_f1macro: 0.1241\n",
      "Epoch 5/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.2750 - f1macro: 0.1414 - val_loss: 0.1219 - val_f1macro: 0.1430\n",
      "Epoch 6/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.2539 - f1macro: 0.1690 - val_loss: 0.1276 - val_f1macro: 0.1788\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 7/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.2398 - f1macro: 0.1897 - val_loss: 0.1078 - val_f1macro: 0.1936 ETA: 6s - loss: 0.2388 -  - ETA: 6s - loss: 0.2392 - f1macro: 0. - ETA: 5s - loss: 0.2400 - f1mac - ETA: 5s - loss: 0.2429 - f1macro - ETA:\n",
      "Epoch 8/50\n",
      "281/281 [==============================] - 21s 76ms/step - loss: 0.2254 - f1macro: 0.2254 - val_loss: 0.1153 - val_f1macro: 0.2429- ETA: 0s - loss: 0.2252 - f1macro: 0.225 - ETA: 0s - loss: 0.2251 - f1macro: 0.22 - ETA: 0s - loss: 0.2251 - f1m\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 9/50\n",
      "281/281 [==============================] - 21s 75ms/step - loss: 0.2120 - f1macro: 0.2689 - val_loss: 0.0970 - val_f1macro: 0.2592: 14s - loss: 0.1989 - f1mac - ETA: 13s - loss: 0.2064 - - ET - ETA: 3s - loss: 0.214\n",
      "Epoch 10/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1983 - f1macro: 0.2800 - val_loss: 0.0978 - val_f1macro: 0.2748: 13s - loss: 0. - ETA: 12s -  - ETA: 11s - loss: 0.1962 - E -  - ETA: 7s - loss: 0.1981 - f1macr - ETA: 4s - loss: 0.199 - ETA: 3s - loss: - ETA: 2s - loss: 0.1965 -  - ETA: 1s - loss: 0.1970 - f1macr - ETA: 0s - loss: 0.1984 - f1m\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 11/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1825 - f1macro: 0.3012 - val_loss: 0.0959 - val_f1macro: 0.2958f1mac - ETA: - ETA: 1s -\n",
      "Epoch 12/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1743 - f1macro: 0.3340 - val_loss: 0.0920 - val_f1macro: 0.3080\n",
      "Epoch 13/50\n",
      "281/281 [==============================] - 21s 75ms/step - loss: 0.1628 - f1macro: 0.3679 - val_loss: 0.0884 - val_f1macro: 0.3108\n",
      "Epoch 14/50\n",
      "281/281 [==============================] - 21s 75ms/step - loss: 0.1565 - f1macro: 0.3714 - val_loss: 0.0942 - val_f1macro: 0.3196: 0.1541 - ETA: 2s - loss: 0.1556 - - ETA: 1s - l\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "Epoch 15/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1377 - f1macro: 0.4083 - val_loss: 0.0921 - val_f1macro: 0.3452\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "Epoch 16/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.1232 - f1macro: 0.4533 - val_loss: 0.0910 - val_f1macro: 0.3416\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "Epoch 17/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.1109 - f1macro: 0.4918 - val_loss: 0.0912 - val_f1macro: 0.3570\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "Epoch 18/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.0999 - f1macro: 0.5069 - val_loss: 0.0970 - val_f1macro: 0.3775\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9fb145fb03644eab1edb27b08d0fd3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=2253)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_on_batch...\n",
      "Class weights...\n",
      "Create model...\n",
      "fit...\n",
      "Epoch 1/50\n",
      "281/281 [==============================] - 22s 75ms/step - loss: 0.3917 - f1macro: 0.0412 - val_loss: 0.1810 - val_f1macro: 0.0197ET\n",
      "Epoch 2/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.3731 - f1macro: 0.0392 - val_loss: 0.1697 - val_f1macro: 0.0318\n",
      "Epoch 3/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.3445 - f1macro: 0.0739 - val_loss: 0.1464 - val_f1macro: 0.0806\n",
      "Epoch 4/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.3145 - f1macro: 0.1021 - val_loss: 0.1334 - val_f1macro: 0.1040\n",
      "Epoch 5/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.2871 - f1macro: 0.1303 - val_loss: 0.1308 - val_f1macro: 0.1179\n",
      "Epoch 6/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.2749 - f1macro: 0.1488 - val_loss: 0.1160 - val_f1macro: 0.1676- loss: 0.2774 - \n",
      "Epoch 7/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.2579 - f1macro: 0.1672 - val_loss: 0.1228 - val_f1macro: 0.1710\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 8/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.2421 - f1macro: 0.1837 - val_loss: 0.1042 - val_f1macro: 0.2012\n",
      "Epoch 9/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.2313 - f1macro: 0.1958 - val_loss: 0.1191 - val_f1macro: 0.1681 0.2302 - f1macro: 0 - ETA - ETA: 2s - loss: 0.2285 - f1macro:\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 10/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.2198 - f1macro: 0.2201 - val_loss: 0.0970 - val_f1macro: 0.2064\n",
      "Epoch 11/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.2083 - f1macro: 0.2426 - val_loss: 0.0977 - val_f1macro: 0.2318\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 12/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1962 - f1macro: 0.2535 - val_loss: 0.0938 - val_f1macro: 0.2453: 0.1918 - f1macro: 0 - ETA: 9s - loss: 0.1919 - - ETA: 8s - loss: 0.1945 - f1macro: 0 - ETA: 7s - loss: 0.1948 - f1macro: 0. - ETA: 7s - loss: 0.1944 - f1macro: 0.253 - ETA: 7s - ETA: 5s - loss: 0.1957 - ETA: 4s - lo - ETA: 2s - loss: 0.195 - ETA: 1s - loss: 0.1957 - f1mac - ETA: 1s - loss: 0.1966 - f1ma - ETA: 0s - loss: 0.1971 - f1mac\n",
      "Epoch 13/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1872 - f1macro: 0.2714 - val_loss: 0.0940 - val_f1macro: 0.2557 loss:  - ETA: 6s - loss: 0.1814 - E - ETA: 3s - loss: 0.1840 - f -\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "Epoch 14/50\n",
      "281/281 [==============================] - 21s 75ms/step - loss: 0.1736 - f1macro: 0.3053 - val_loss: 0.0958 - val_f1macro: 0.2681A: 4s - loss: 0. - ETA: 2s - loss: 0.1740 - f1 - ETA: 1s \n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "Epoch 15/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1583 - f1macro: 0.3375 - val_loss: 0.0912 - val_f1macro: 0.2903s - loss: 0.157\n",
      "Epoch 16/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1486 - f1macro: 0.3672 - val_loss: 0.0917 - val_f1macro: 0.3138\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "Epoch 17/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.1374 - f1macro: 0.4080 - val_loss: 0.0913 - val_f1macro: 0.3059\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "Epoch 18/50\n",
      "281/281 [==============================] - 21s 75ms/step - loss: 0.1268 - f1macro: 0.4203 - val_loss: 0.0953 - val_f1macro: 0.2825\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "Epoch 19/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1172 - f1macro: 0.4303 - val_loss: 0.0951 - val_f1macro: 0.2964\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "Epoch 20/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1100 - f1macro: 0.4410 - val_loss: 0.0954 - val_f1macro: 0.3042\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
      "Epoch 00020: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9d7d6b3d654bad8056038ad193cef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=2253)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_on_batch...\n",
      "Class weights...\n",
      "Create model...\n",
      "fit...\n",
      "Epoch 1/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.3971 - f1macro: 0.0409 - val_loss: 0.1726 - val_f1macro: 0.0285\n",
      "Epoch 2/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.3715 - f1macro: 0.0392 - val_loss: 0.1669 - val_f1macro: 0.0286\n",
      "Epoch 3/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.3414 - f1macro: 0.0585 - val_loss: 0.1371 - val_f1macro: 0.0702\n",
      "Epoch 4/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.3188 - f1macro: 0.0846 - val_loss: 0.1611 - val_f1macro: 0.0683\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 5/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.2955 - f1macro: 0.0995 - val_loss: 0.1365 - val_f1macro: 0.0986\n",
      "Epoch 6/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.2737 - f1macro: 0.1320 - val_loss: 0.1121 - val_f1macro: 0.1561\n",
      "Epoch 7/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.2531 - f1macro: 0.1669 - val_loss: 0.1085 - val_f1macro: 0.1608\n",
      "Epoch 8/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.2440 - f1macro: 0.1843 - val_loss: 0.1083 - val_f1macro: 0.1663\n",
      "Epoch 9/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.2288 - f1macro: 0.2057 - val_loss: 0.1099 - val_f1macro: 0.1817\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 10/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.2155 - f1macro: 0.2135 - val_loss: 0.0977 - val_f1macro: 0.2070s: 0.\n",
      "Epoch 11/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.2069 - f1macro: 0.2351 - val_loss: 0.1046 - val_f1macro: 0.2171\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 12/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1925 - f1macro: 0.2555 - val_loss: 0.0963 - val_f1macro: 0.2286\n",
      "Epoch 13/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1830 - f1macro: 0.2730 - val_loss: 0.0949 - val_f1macro: 0.2618\n",
      "Epoch 14/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.1740 - f1macro: 0.2916 - val_loss: 0.0922 - val_f1macro: 0.2671\n",
      "Epoch 15/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1649 - f1macro: 0.3063 - val_loss: 0.0944 - val_f1macro: 0.2638\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "Epoch 16/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1503 - f1macro: 0.3403 - val_loss: 0.0943 - val_f1macro: 0.2680\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "Epoch 17/50\n",
      "281/281 [==============================] - 21s 73ms/step - loss: 0.1366 - f1macro: 0.3831 - val_loss: 0.0922 - val_f1macro: 0.2980\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "Epoch 18/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1247 - f1macro: 0.4462 - val_loss: 0.0955 - val_f1macro: 0.2879\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "Epoch 19/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1139 - f1macro: 0.4715 - val_loss: 0.0988 - val_f1macro: 0.3003\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "Epoch 00019: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160016331dca4f1880adcff395086c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=2253)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_on_batch...\n",
      "Class weights...\n",
      "Create model...\n",
      "fit...\n",
      "Epoch 1/50\n",
      "281/281 [==============================] - 23s 79ms/step - loss: 0.3966 - f1macro: 0.0407 - val_loss: 0.1717 - val_f1macro: 0.01940.4816 - f1macro: 0.03 - ETA: 14s - loss:  - ETA - ETA: 10s - loss: 0.4107 - f1macro: 0.04 - ETA: 10s - l - ETA: 5s - loss: 0.3998 - f1macro:  - ETA: 5s - loss: 0.3990 - f1macro: 0 - ETA: 5s - loss: 0.3989 - f1macro: 0 - ETA: 4s - loss: 0.3978  - ETA: 3s - loss: 0.3986 - f1m - ETA: 3s - loss: 0 - ETA: 1s - \n",
      "Epoch 2/50\n",
      "281/281 [==============================] - 21s 76ms/step - loss: 0.3683 - f1macro: 0.0467 - val_loss: 0.1749 - val_f1macro: 0.0455\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 3/50\n",
      "281/281 [==============================] - 21s 76ms/step - loss: 0.3344 - f1macro: 0.0798 - val_loss: 0.1450 - val_f1macro: 0.0910- loss: 0.3411 - f1macro: 0. - ETA: 9s - loss: 0 - ETA: 8s - los - ETA: 4s - loss: 0.3360 - f1macro: 0.076 - ETA: 4s - loss: 0.3361 - ETA: 2s - loss: 0.3344 - f1macro: 0.077 - ETA: 2s - loss: 0.334 - ETA: 1s - loss: 0. - ETA: 0s - loss: 0.3350 - f1macro: 0\n",
      "Epoch 4/50\n",
      "281/281 [==============================] - 21s 76ms/step - loss: 0.3025 - f1macro: 0.1039 - val_loss: 0.1219 - val_f1macro: 0.1147\n",
      "Epoch 5/50\n",
      "281/281 [==============================] - 22s 77ms/step - loss: 0.2787 - f1macro: 0.1268 - val_loss: 0.1111 - val_f1macro: 0.1400\n",
      "Epoch 6/50\n",
      "281/281 [==============================] - 21s 75ms/step - loss: 0.2574 - f1macro: 0.1590 - val_loss: 0.1169 - val_f1macro: 0.1671\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 7/50\n",
      "281/281 [==============================] - 21s 75ms/step - loss: 0.2396 - f1macro: 0.1923 - val_loss: 0.1064 - val_f1macro: 0.1815acro: 0.18 - ETA: 11s - lo - ETA: 10s - loss: 0.2382 -  - ETA: 9s - loss: 0.2395 - f1macro: - ETA: 3s - lo - ET\n",
      "Epoch 8/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.2290 - f1macro: 0.2108 - val_loss: 0.1043 - val_f1macro: 0.1968\n",
      "Epoch 9/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.2174 - f1macro: 0.2205 - val_loss: 0.1012 - val_f1macro: 0.2179\n",
      "Epoch 10/50\n",
      "281/281 [==============================] - 21s 75ms/step - loss: 0.2114 - f1macro: 0.2424 - val_loss: 0.0999 - val_f1macro: 0.2262: 1s - \n",
      "Epoch 11/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1989 - f1macro: 0.2624 - val_loss: 0.1044 - val_f1macro: 0.2308\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 12/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1843 - f1macro: 0.2926 - val_loss: 0.1002 - val_f1macro: 0.2476 - loss: 0.1829 -  - ETA: 0s - loss: 0.1833 - f1macro: \n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "Epoch 13/50\n",
      "281/281 [==============================] - 21s 75ms/step - loss: 0.1685 - f1macro: 0.3122 - val_loss: 0.0963 - val_f1macro: 0.2825A: 3s - loss: 0.1686 - f1macro: - ETA: 3s - loss: 0.1689 - f1macro: 0.305 - ETA: 3s - loss: 0.1689 - f1ma - ETA: 2s - loss: 0.1691 - f1macro: 0 - ETA: 2s - loss: 0.1687 - f1macro - ETA: 1s - los\n",
      "Epoch 14/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1587 - f1macro: 0.3449 - val_loss: 0.1035 - val_f1macro: 0.2823\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "Epoch 15/50\n",
      "281/281 [==============================] - 21s 74ms/step - loss: 0.1485 - f1macro: 0.3796 - val_loss: 0.0971 - val_f1macro: 0.3048\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "Epoch 16/50\n",
      "281/281 [==============================] - 21s 75ms/step - loss: 0.1348 - f1macro: 0.4062 - val_loss: 0.1003 - val_f1macro: 0.3163\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "Epoch 17/50\n",
      "281/281 [==============================] - 21s 76ms/step - loss: 0.1259 - f1macro: 0.4267 - val_loss: 0.0979 - val_f1macro: 0.3063: 6s - loss: 0.1262 - f1macro: - ETA: 5s - loss: 0.1253 - f1ma - ETA: 2 - ETA: 0s - loss: 0.1260 - f1macro: 0.42\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "Epoch 18/50\n",
      "281/281 [==============================] - 21s 75ms/step - loss: 0.1156 - f1macro: 0.4433 - val_loss: 0.0995 - val_f1macro: 0.3119ETA: 3s - loss: 0.1144 - f1macro - ETA: 0s - loss: 0.1165 - f1ma\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64153be2f34a4f4797184bcb6c22cc65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=2252)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_on_batch...\n"
     ]
    }
   ],
   "source": [
    "# collect out of sample predictions\n",
    "VGGish_yhat = {}\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_ix, test_ix in kfold.split(X_train):\n",
    "    # get data\n",
    "    train_X, test_X = X_train.iloc[train_ix], X_train.iloc[test_ix]\n",
    "\n",
    "    # Instanciate data generators\n",
    "    train_generator = DataGenerator_VGGish(train_X, **params_train)\n",
    "    test_generator = DataGenerator_VGGish(test_X, **params_train)\n",
    "\n",
    "    # Class weights\n",
    "    print('Class weights...')\n",
    "    class_weights = class_weight(generator=train_generator, mu=0.675)\n",
    "\n",
    "    # Create VGGish model\n",
    "    print('Create model...')\n",
    "    VGGish = create_cnn()\n",
    "    VGGish.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[tfa.metrics.F1Score(name='f1macro', num_classes=len(labels), average='macro')])\n",
    "\n",
    "    print('fit...')\n",
    "    VGGish.fit(\n",
    "        train_generator,\n",
    "        validation_data=test_generator,\n",
    "        epochs=50,\n",
    "        callbacks=[es_callback, reduce_lr],\n",
    "        verbose=1,\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "\n",
    "    # Predict & store\n",
    "    # Instantiate the progress bar\n",
    "    max_count = test_generator.X.shape[0]\n",
    "    f = IntProgress(min=0, max=max_count)\n",
    "    # Display the progress bar\n",
    "    display(f)\n",
    "\n",
    "    print('predict_on_batch...')\n",
    "    for index, row in test_generator.X.iterrows():\n",
    "        # Increment the progress bar\n",
    "        f.value += 1\n",
    "        # Format data\n",
    "        X = np.empty((1, 5, 96, 64))\n",
    "        X[0] = np.array(data_mem[row['filename']])\n",
    "        X = X.reshape(1, 480, 64, 1)\n",
    "        # Predict\n",
    "        pred = VGGish.predict_on_batch(X)\n",
    "        # Store prediction\n",
    "        VGGish_yhat[index] = pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c12d5d",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cb6d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGGish.save_weights(WORKING_PATH + 'VGGish.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bdfa04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./working/stacking/VGGish_yhat.jl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(VGGish_yhat, WORKING_PATH + 'VGGish_yhat.jl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e5187",
   "metadata": {},
   "source": [
    "## Meta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a25f337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a meta dataset\n",
    "def create_meta_dataset(data_x, yhat1, yhat2, yhat3):\n",
    "    # convert to dataframes\n",
    "    df_new1 = pd.DataFrame.from_dict(yhat1, orient='index', columns=['tr1', 'tr2', 'tr3', 'tr4', 'tr5',\n",
    "                                                                     'tr6', 'tr7', 'tr8', 'tr9', 'tr10',\n",
    "                                                                     'tr11', 'tr12', 'tr13', 'tr14', 'tr15',\n",
    "                                                                     'tr16', 'tr17', 'tr18', 'tr19', 'tr20',\n",
    "                                                                     'tr21'])\n",
    "    \n",
    "    df_new2 = pd.DataFrame.from_dict(yhat2, orient='index', columns=['en1', 'en2', 'en3', 'en4', 'en5',\n",
    "                                                                     'en6', 'en7', 'en8', 'en9', 'en10',\n",
    "                                                                     'en11', 'en12', 'en13', 'en14', 'en15',\n",
    "                                                                     'en16', 'en17', 'en18', 'en19', 'en20',\n",
    "                                                                     'en21'])\n",
    "    \n",
    "    df_new3 = pd.DataFrame.from_dict(yhat3, orient='index', columns=['vg1', 'vg2', 'vg3', 'vg4', 'vg5',\n",
    "                                                                     'vg6', 'vg7', 'vg8', 'vg9', 'vg10',\n",
    "                                                                     'vg11', 'vg12', 'vg13', 'vg14', 'vg15',\n",
    "                                                                     'vg16', 'vg17', 'vg18', 'vg19', 'vg20',\n",
    "                                                                     'vg21'])\n",
    "    # create a meta dataset\n",
    "    X = pd.concat([data_x, df_new1, df_new2, df_new3], axis=1, verify_integrity=True)\n",
    "    y = mlb.transform(X['target'])\n",
    "\n",
    "    X = X.drop(['primary_label', 'secondary_labels',\n",
    "                'original_filename', 'filename', 'target'], axis=1)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76f69612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload yhats\n",
    "trill_yhat = joblib.load(WORKING_PATH + 'trill_yhat.jl')\n",
    "EfficientNetB0_yhat = joblib.load(WORKING_PATH + 'EfficientNetB0_yhat.jl')\n",
    "VGGish_yhat = joblib.load(WORKING_PATH + 'VGGish_yhat.jl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "930d34bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct meta dataset\n",
    "meta_X_train, meta_y_train = create_meta_dataset(X_train, trill_yhat, EfficientNetB0_yhat, VGGish_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39a7254e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct meta classifier\n",
    "meta_model = RandomForestClassifier()\n",
    "meta_model.fit(meta_X_train, meta_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "306b65d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./working/stacking/meta_model.jl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "joblib.dump(meta_model, WORKING_PATH + 'meta_model.jl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71f381d",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd29028",
   "metadata": {},
   "source": [
    "### Sub models on hold out dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a297c4",
   "metadata": {},
   "source": [
    "#### Trill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a500b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_generator_trill = DataGenerator_trill(X_valid, **params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22f1c3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine_tune_at == None\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, None, 2048)        51964864  \n",
      "_________________________________________________________________\n",
      "net_vlad (NetVLAD)           (None, 16384)             32776     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 16384)             65536     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               4194560   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 21)                5397      \n",
      "=================================================================\n",
      "Total params: 56,263,133\n",
      "Trainable params: 4,265,501\n",
      "Non-trainable params: 51,997,632\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Trill = create_cnn(num_clusters=8, use_batchnorm=True,\n",
    "                   pooling=None, hidden=256,\n",
    "                   fine_tune_at=None, model_path=None)\n",
    "Trill.load_weights(WORKING_PATH + 'trill.h5')\n",
    "Trill.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tfa.metrics.F1Score(name='f1macro', num_classes=len(labels), average='macro')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43aac2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.034373924136161804, 0.5638612508773804]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_trill = Trill.evaluate_generator(valid_generator_trill)\n",
    "pred_trill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57411fb",
   "metadata": {},
   "source": [
    "#### EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c70cac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_generator_EfficientNetB0 = DataGenerator_EfficientNetB0(X_valid, **params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "588a1956",
   "metadata": {},
   "outputs": [],
   "source": [
    "EfficientNetB0 = create_cnn()\n",
    "EfficientNetB0.load_weights(WORKING_PATH + 'EfficientNetB0.h5')\n",
    "EfficientNetB0.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=[tfa.metrics.F1Score(name='f1macro', num_classes=len(labels), average='macro')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f2c14f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0552985779941082, 0.4963090121746063]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_EfficientNetB0 = EfficientNetB0.evaluate_generator(valid_generator_EfficientNetB0)\n",
    "pred_EfficientNetB0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b10825",
   "metadata": {},
   "source": [
    "#### VGGish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7230dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_generator_VGGish = DataGenerator_VGGish(X_valid, **params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d51688fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGGish = create_cnn()\n",
    "VGGish.load_weights(WORKING_PATH + 'VGGish.h5')\n",
    "VGGish.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=[tfa.metrics.F1Score(name='f1macro', num_classes=len(labels), average='macro')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29e8a859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09559944272041321, 0.2858201563358307]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_VGGish = VGGish.evaluate_generator(valid_generator_VGGish)\n",
    "pred_VGGish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced932e",
   "metadata": {},
   "source": [
    "### Meta model on hold out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6211025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload meta_model\n",
    "meta_model = joblib.load(WORKING_PATH + 'meta_model.jl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b391c8b",
   "metadata": {},
   "source": [
    "#### Trill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82b9ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-execute first the cells corresponding to the preprocessing functions for this model\n",
    "# Sound noise reduction\n",
    "def f_high(y,sr):\n",
    "    b,a = signal.butter(10, 1000/(sr/2), btype='highpass')\n",
    "    yf = signal.lfilter(b,a,y)\n",
    "    return yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba3305dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(y, sr):\n",
    "    # Sound noise reduction\n",
    "    y = f_high(y, sr)\n",
    "    # Resample\n",
    "    y = librosa.resample(y, sr, 16000)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe348551",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    batch_size=32,\n",
    "    n_rows=224,\n",
    "    n_columns=216,\n",
    "    n_channels=3,\n",
    ")\n",
    "params_train = dict(\n",
    "    shuffle=False,\n",
    "    **params\n",
    ")\n",
    "params_valid = dict(\n",
    "    shuffle=False,\n",
    "    **params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a18f61ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a12681ddc884497a3a096d246e777b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=14080)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data in RAM to speed up training process\n",
    "data_mem.clear()\n",
    "data_mem = LoadRAM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32c99cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_pred_trill = {}\n",
    "\n",
    "for index, row in valid_generator_trill.X.iterrows():\n",
    "    pred = Trill.predict_on_batch(data_mem[row['filename']].reshape(1, -1))\n",
    "    meta_pred_trill[index] = pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4074dc5c",
   "metadata": {},
   "source": [
    "#### EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "965f5bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-execute first the cells corresponding to the preprocessing functions for this model\n",
    "class conf:\n",
    "    # Preprocessing settings\n",
    "    sampling_rate = 44100\n",
    "    n_mels = 224\n",
    "    hop_length = 494\n",
    "    n_fft = n_mels * 10\n",
    "    fmin = 20\n",
    "    fmax = 16000\n",
    "    \n",
    "    # Model parameters\n",
    "    num_rows = 224\n",
    "    num_columns = 224\n",
    "    num_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cef79d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_melspectrogram(audio):\n",
    "    spectrogram = librosa.feature.melspectrogram(audio,\n",
    "                                                 sr=conf.sampling_rate,\n",
    "                                                 n_mels=conf.n_mels,\n",
    "                                                 hop_length=conf.hop_length,\n",
    "                                                 n_fft=conf.n_fft,\n",
    "                                                 fmin=conf.fmin,\n",
    "                                                 fmax=conf.fmax)\n",
    "    spectrogram = librosa.power_to_db(spectrogram)\n",
    "    spectrogram = spectrogram.astype(np.float32)\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45095495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mono_to_color(X, eps=1e-6, mean=None, std=None):\n",
    "    \"\"\"\n",
    "    Converts a one channel array to a 3 channel one in [0, 255]\n",
    "    Arguments:\n",
    "        X {numpy array [H x W]} -- 2D array to convert\n",
    "    Keyword Arguments:\n",
    "        eps {float} -- To avoid dividing by 0 (default: {1e-6})\n",
    "        mean {None or np array} -- Mean for normalization (default: {None})\n",
    "        std {None or np array} -- Std for normalization (default: {None})\n",
    "    Returns:\n",
    "        numpy array [3 x H x W] -- RGB numpy array\n",
    "    \"\"\"\n",
    "    X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Standardize\n",
    "    mean = mean or X.mean()\n",
    "    std = std or X.std()\n",
    "    X = (X - mean) / (std + eps)\n",
    "\n",
    "    # Normalize to [0, 255]\n",
    "    _min, _max = X.min(), X.max()\n",
    "\n",
    "    if (_max - _min) > eps:\n",
    "        V = np.clip(X, _min, _max)\n",
    "        V = 255 * (V - _min) / (_max - _min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        V = np.zeros_like(X, dtype=np.uint8)\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c0618403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(y, sr):\n",
    "    # Extract features\n",
    "    feat = audio_to_melspectrogram(y)\n",
    "    feat = mono_to_color(feat)\n",
    "    feat = feat.astype(np.uint8)\n",
    "    \n",
    "    # EfficientNet preprocess\n",
    "    feat = preprocess_input(feat)\n",
    "    \n",
    "    X = np.empty((1, conf.num_rows, conf.num_columns, conf.num_channels))\n",
    "    x_features = feat.tolist()\n",
    "    X[0] = np.array(x_features)\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b432dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    batch_size=16,\n",
    "    n_rows=conf.num_rows,\n",
    "    n_columns=conf.num_columns,\n",
    "    n_channels=conf.num_channels,\n",
    ")\n",
    "params_train = dict(\n",
    "    shuffle=False,\n",
    "    **params\n",
    ")\n",
    "params_valid = dict(\n",
    "    shuffle=False,\n",
    "    **params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eece8cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717385e07d294d1cbc59328d050beac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=14080)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data in RAM to speed up training process\n",
    "data_mem.clear()\n",
    "data_mem = LoadRAM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "acfb4655",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_pred_EfficientNetB0 = {}\n",
    "\n",
    "for index, row in valid_generator_EfficientNetB0.X.iterrows():\n",
    "    # Format data\n",
    "    X = np.empty((1, conf.num_rows, conf.num_columns, conf.num_channels))\n",
    "    X[0] = np.array(data_mem[row['filename']])\n",
    "    # Predict\n",
    "    pred = EfficientNetB0.predict_on_batch(X)\n",
    "    # Store prediction\n",
    "    meta_pred_EfficientNetB0[index] = pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c42f8f7",
   "metadata": {},
   "source": [
    "#### VGGish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ea48487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-execute first the cells corresponding to the preprocessing functions for this model\n",
    "# Sound noise reduction\n",
    "def f_high(y,sr):\n",
    "    b,a = signal.butter(10, 2000/(sr/2), btype='highpass')\n",
    "    yf = signal.lfilter(b,a,y)\n",
    "    return yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ce05a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(y, sr):\n",
    "    # Sound noise reduction\n",
    "    y = f_high(y, sr)\n",
    "    \n",
    "    feat = vggish_input.waveform_to_examples(y, sr)\n",
    "        \n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "60848459",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    batch_size=32,\n",
    "    n_rows=5,\n",
    "    n_columns=96,\n",
    "    n_channels=64,\n",
    ")\n",
    "params_train = dict(\n",
    "    shuffle=True,\n",
    "    **params\n",
    ")\n",
    "params_valid = dict(\n",
    "    shuffle=False,\n",
    "    **params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15b3f2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487d107d6d134e0899cb1d78d20f154f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=14080)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data in RAM to speed up training process\n",
    "data_mem.clear()\n",
    "data_mem = LoadRAM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9fb8d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_pred_VGGish = {}\n",
    "\n",
    "for index, row in valid_generator_VGGish.X.iterrows():\n",
    "    # Format data\n",
    "    X = np.empty((1, 5, 96, 64))\n",
    "    X[0] = np.array(data_mem[row['filename']])\n",
    "    X = X.reshape(1, 480, 64, 1)\n",
    "    # Predict\n",
    "    pred = VGGish.predict_on_batch(X)\n",
    "    # Store prediction\n",
    "    meta_pred_VGGish[index] = pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5111497b",
   "metadata": {},
   "source": [
    "#### Save point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "78ffc5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./working/stacking/meta_pred_VGGish.jl']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(meta_pred_trill, WORKING_PATH + 'meta_pred_trill.jl')\n",
    "joblib.dump(meta_pred_EfficientNetB0, WORKING_PATH + 'meta_pred_EfficientNetB0.jl')\n",
    "joblib.dump(meta_pred_VGGish, WORKING_PATH + 'meta_pred_VGGish.jl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e886a0",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4bcabd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct meta dataset\n",
    "meta_X_valid, meta_y_valid = create_meta_dataset(X_valid, meta_pred_trill, meta_pred_EfficientNetB0, meta_pred_VGGish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4f4a4720",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_meta = meta_model.predict(meta_X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "646f0dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7392820767883205\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(meta_y_valid, pred_meta, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc3940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfp8",
   "language": "python",
   "name": "tfp8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
